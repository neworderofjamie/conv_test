{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.nn.functional import avg_pool2d, conv2d \n",
    "from scipy.linalg import toeplitz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 64\n",
    "INPUT_CHANNELS = 1\n",
    "OUTPUT_CHANNELS = 1\n",
    "\n",
    "if INPUT_CHANNELS == 1:\n",
    "    spike_indices = np.asarray([65,   66,   67,   68,   69,   70,   71,   72,   73,   74,   75,\n",
    "          76,   77,   78,   79,   80,   81,   82,   83,   84,   85,   86,\n",
    "          87,   88,   89,   90,   91,   92,   93,   94,   95,   96,   97,\n",
    "          98,   99,  100,  101,  102,  103,  104,  105,  106,  107,  108,\n",
    "         109,  110,  129,  130,  131,  132,  133,  134,  135,  136,  137,\n",
    "         138,  139,  140,  141,  142,  143,  144,  145,  146,  147,  148,\n",
    "         149,  150,  151,  152,  153,  154,  155,  156,  157,  158,  159,\n",
    "         160,  161,  162,  163,  164,  165,  166,  167,  168,  169,  170,\n",
    "         171,  172,  173,  174,  193,  194,  195,  196,  197,  198,  199,\n",
    "         200,  201,  202,  203,  204,  205,  206,  207,  208,  209,  210,\n",
    "         211,  212,  213,  214,  215,  257,  258,  259,  260,  261,  262,\n",
    "         263,  264,  265,  266,  267,  268,  269,  270,  271,  272,  273,\n",
    "         274,  275,  276,  277,  278,  279,  321,  322,  323,  324,  325,\n",
    "         326,  327,  328,  329,  330,  331,  332,  333,  334,  335,  336,\n",
    "         337,  338,  339,  340,  341,  342,  343,  385,  386,  387,  388,\n",
    "         389,  390,  391,  392,  393,  394,  395,  396,  397,  398,  399,\n",
    "         400,  401,  402,  403,  404,  405,  406,  407,  449,  450,  451,\n",
    "         452,  453,  454,  455,  456,  457,  458,  459,  460,  461,  462,\n",
    "         463,  464,  465,  466,  467,  468,  469,  470,  471,  507,  508,\n",
    "         513,  514,  515,  516,  517,  518,  519,  520,  521,  522,  523,\n",
    "         524,  525,  526,  527,  528,  529,  530,  531,  532,  533,  534,\n",
    "         535,  568,  569,  570,  571,  577,  578,  579,  580,  581,  582,\n",
    "         583,  584,  585,  586,  587,  588,  589,  590,  591,  592,  593,\n",
    "         594,  595,  596,  597,  598,  599,  629,  630,  631,  632,  633,\n",
    "         634,  641,  642,  643,  644,  645,  646,  647,  648,  649,  650,\n",
    "         651,  652,  653,  654,  655,  656,  657,  658,  659,  660,  661,\n",
    "         662,  663,  690,  691,  692,  693,  694,  695,  696,  697,  705,\n",
    "         706,  707,  708,  709,  710,  711,  712,  713,  714,  715,  716,\n",
    "         717,  718,  719,  720,  721,  722,  723,  724,  725,  726,  727,\n",
    "         750,  751,  752,  753,  754,  755,  756,  757,  758,  759,  760,\n",
    "         769,  770,  771,  772,  773,  774,  775,  776,  777,  778,  779,\n",
    "         780,  781,  782,  783,  784,  785,  786,  787,  788,  789,  790,\n",
    "         791,  810,  811,  812,  813,  814,  815,  816,  817,  818,  819,\n",
    "         820,  821,  822,  823,  833,  834,  835,  836,  837,  838,  839,\n",
    "         840,  841,  842,  843,  844,  845,  846,  847,  848,  849,  850,\n",
    "         851,  852,  853,  854,  855,  871,  872,  873,  874,  875,  876,\n",
    "         877,  878,  879,  880,  881,  882,  883,  884,  885,  886,  897,\n",
    "         898,  899,  900,  901,  902,  903,  904,  905,  906,  907,  908,\n",
    "         909,  910,  911,  912,  913,  914,  915,  916,  917,  918,  919,\n",
    "         932,  933,  934,  935,  936,  937,  938,  939,  940,  941,  942,\n",
    "         943,  944,  945,  946,  947,  948,  949,  992,  993,  994,  995,\n",
    "         996,  997,  998,  999, 1000, 1001, 1002, 1003, 1004, 1005, 1006,\n",
    "        1007, 1008, 1009, 1010, 1011, 1012, 1053, 1054, 1055, 1056, 1057,\n",
    "        1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068,\n",
    "        1069, 1070, 1071, 1072, 1073, 1074, 1075, 1114, 1115, 1116, 1117,\n",
    "        1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128,\n",
    "        1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1174,\n",
    "        1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185,\n",
    "        1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196,\n",
    "        1197, 1198, 1199, 1200, 1201, 1217, 1218, 1219, 1220, 1221, 1222,\n",
    "        1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1235,\n",
    "        1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246,\n",
    "        1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257,\n",
    "        1258, 1259, 1260, 1261, 1262, 1263, 1264, 1281, 1282, 1283, 1284,\n",
    "        1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295,\n",
    "        1296, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308,\n",
    "        1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319,\n",
    "        1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1345, 1346,\n",
    "        1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357,\n",
    "        1358, 1359, 1360, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373,\n",
    "        1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384,\n",
    "        1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1409, 1410,\n",
    "        1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421,\n",
    "        1422, 1423, 1424, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440,\n",
    "        1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451,\n",
    "        1452, 1453, 1454, 1455, 1456, 1457, 1458, 1473, 1474, 1475, 1476,\n",
    "        1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487,\n",
    "        1488, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510,\n",
    "        1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521,\n",
    "        1522, 1523, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545,\n",
    "        1546, 1547, 1548, 1549, 1550, 1551, 1552, 1568, 1569, 1570, 1571,\n",
    "        1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582,\n",
    "        1583, 1584, 1585, 1586, 1587, 1588, 1601, 1602, 1603, 1604, 1605,\n",
    "        1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616,\n",
    "        1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646,\n",
    "        1647, 1648, 1649, 1650, 1651, 1652, 1653, 1665, 1666, 1667, 1668,\n",
    "        1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679,\n",
    "        1680, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712,\n",
    "        1713, 1714, 1715, 1716, 1717, 1718, 1729, 1730, 1731, 1732, 1733,\n",
    "        1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744,\n",
    "        1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780,\n",
    "        1781, 1782, 1783, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800,\n",
    "        1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1837, 1838, 1839,\n",
    "        1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1857, 1858,\n",
    "        1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869,\n",
    "        1870, 1871, 1872, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911,\n",
    "        1912, 1913, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929,\n",
    "        1930, 1931, 1932, 1933, 1934, 1935, 1936, 1972, 1973, 1974, 1975,\n",
    "        1976, 1977, 1978, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992,\n",
    "        1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2039, 2040, 2041,\n",
    "        2042, 2043, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057,\n",
    "        2058, 2059, 2060, 2061, 2062, 2063, 2064, 2106, 2107, 2108, 2267,\n",
    "        2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278,\n",
    "        2279, 2280, 2281, 2282, 2283, 2284, 2285, 2286, 2287, 2288, 2305,\n",
    "        2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2314, 2315, 2316,\n",
    "        2317, 2318, 2319, 2320, 2321, 2322, 2323, 2331, 2332, 2333, 2334,\n",
    "        2335, 2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2344, 2345,\n",
    "        2346, 2347, 2348, 2349, 2350, 2351, 2352, 2370, 2371, 2372, 2373,\n",
    "        2374, 2375, 2376, 2377, 2378, 2379, 2380, 2381, 2382, 2383, 2384,\n",
    "        2385, 2386, 2387, 2395, 2396, 2397, 2398, 2399, 2400, 2401, 2402,\n",
    "        2403, 2404, 2405, 2406, 2407, 2408, 2409, 2410, 2411, 2412, 2413,\n",
    "        2414, 2415, 2416, 2435, 2436, 2437, 2438, 2439, 2440, 2441, 2442,\n",
    "        2443, 2444, 2445, 2446, 2447, 2448, 2449, 2450, 2451, 2459, 2460,\n",
    "        2461, 2462, 2463, 2464, 2465, 2466, 2467, 2468, 2469, 2470, 2471,\n",
    "        2472, 2473, 2474, 2475, 2476, 2477, 2478, 2479, 2480, 2500, 2501,\n",
    "        2502, 2503, 2504, 2505, 2506, 2507, 2508, 2509, 2510, 2511, 2512,\n",
    "        2513, 2514, 2515, 2523, 2524, 2525, 2526, 2527, 2528, 2529, 2530,\n",
    "        2531, 2532, 2533, 2534, 2535, 2536, 2537, 2538, 2539, 2540, 2541,\n",
    "        2542, 2543, 2544, 2565, 2566, 2567, 2568, 2569, 2570, 2571, 2572,\n",
    "        2573, 2574, 2575, 2576, 2577, 2578, 2579, 2587, 2588, 2589, 2590,\n",
    "        2591, 2592, 2593, 2594, 2595, 2596, 2597, 2598, 2599, 2600, 2601,\n",
    "        2602, 2603, 2604, 2605, 2606, 2607, 2608, 2630, 2631, 2632, 2633,\n",
    "        2634, 2635, 2636, 2637, 2638, 2639, 2640, 2641, 2642, 2643, 2651,\n",
    "        2652, 2653, 2654, 2655, 2656, 2657, 2658, 2659, 2660, 2661, 2662,\n",
    "        2663, 2664, 2665, 2666, 2667, 2668, 2669, 2670, 2671, 2672, 2695,\n",
    "        2696, 2697, 2698, 2699, 2700, 2701, 2702, 2703, 2704, 2705, 2706,\n",
    "        2707, 2715, 2716, 2717, 2718, 2719, 2720, 2721, 2722, 2723, 2724,\n",
    "        2725, 2726, 2727, 2728, 2729, 2730, 2731, 2732, 2733, 2734, 2735,\n",
    "        2736, 2760, 2761, 2762, 2763, 2764, 2765, 2766, 2767, 2768, 2769,\n",
    "        2770, 2771, 2779, 2780, 2781, 2782, 2783, 2784, 2785, 2786, 2787,\n",
    "        2788, 2789, 2790, 2791, 2792, 2793, 2794, 2795, 2796, 2797, 2798,\n",
    "        2799, 2800, 2825, 2826, 2827, 2828, 2829, 2830, 2831, 2832, 2833,\n",
    "        2834, 2835, 2843, 2844, 2845, 2846, 2847, 2848, 2849, 2850, 2851,\n",
    "        2852, 2853, 2854, 2855, 2856, 2857, 2858, 2859, 2860, 2861, 2862,\n",
    "        2863, 2864, 2890, 2891, 2892, 2893, 2894, 2895, 2896, 2897, 2898,\n",
    "        2899, 2907, 2908, 2909, 2910, 2911, 2912, 2913, 2914, 2915, 2916,\n",
    "        2917, 2918, 2919, 2920, 2921, 2922, 2923, 2924, 2925, 2926, 2927,\n",
    "        2928, 2955, 2956, 2957, 2958, 2959, 2960, 2961, 2962, 2963, 2971,\n",
    "        2972, 2973, 2974, 2975, 2976, 2977, 2978, 2979, 2980, 2981, 2982,\n",
    "        2983, 2984, 2985, 2986, 2987, 2988, 2989, 2990, 2991, 2992, 3020,\n",
    "        3021, 3022, 3023, 3024, 3025, 3026, 3027, 3035, 3036, 3037, 3038,\n",
    "        3039, 3040, 3041, 3042, 3043, 3044, 3045, 3046, 3047, 3048, 3049,\n",
    "        3050, 3051, 3052, 3053, 3054, 3055, 3056, 3085, 3086, 3087, 3088,\n",
    "        3089, 3090, 3091, 3099, 3100, 3101, 3102, 3103, 3104, 3105, 3106,\n",
    "        3107, 3108, 3109, 3110, 3111, 3112, 3113, 3114, 3115, 3116, 3117,\n",
    "        3118, 3119, 3120, 3150, 3151, 3152, 3153, 3154, 3155, 3215, 3216,\n",
    "        3217, 3218, 3219, 3280, 3281, 3282, 3283, 3345, 3346, 3347, 3410,\n",
    "        3411, 3475, 3547, 3548, 3549, 3550, 3551, 3608, 3609, 3610, 3611,\n",
    "        3612, 3613, 3614, 3615, 3616, 3617, 3618, 3669, 3670, 3671, 3672,\n",
    "        3673, 3674, 3675, 3676, 3677, 3678, 3679, 3680, 3681, 3682, 3683,\n",
    "        3684, 3685, 3686, 3730, 3731, 3732, 3733, 3734, 3735, 3736, 3737,\n",
    "        3738, 3739, 3740, 3741, 3742, 3743, 3744, 3745, 3746, 3747, 3748,\n",
    "        3749, 3750, 3751, 3752, 3753, 3790, 3791, 3792, 3793, 3794, 3795,\n",
    "        3796, 3797, 3798, 3799, 3800, 3801, 3802, 3803, 3804, 3805, 3806,\n",
    "        3807, 3808, 3809, 3810, 3811, 3812, 3813, 3814, 3815, 3816, 3817,\n",
    "        3818, 3819, 3820, 3851, 3852, 3853, 3854, 3855, 3856, 3857, 3858,\n",
    "        3859, 3860, 3861, 3862, 3863, 3864, 3865, 3866, 3867, 3868, 3869,\n",
    "        3870, 3871, 3872, 3873, 3874, 3875, 3876, 3877, 3878, 3879, 3880,\n",
    "        3881, 3882, 3883, 3884, 3885, 3886, 3887, 3912, 3913, 3914, 3915,\n",
    "        3916, 3917, 3918, 3919, 3920, 3921, 3922, 3923, 3924, 3925, 3926,\n",
    "        3927, 3928, 3929, 3930, 3931, 3932, 3933, 3934, 3935, 3936, 3937,\n",
    "        3938, 3939, 3940, 3941, 3942, 3943, 3944, 3945, 3946, 3947, 3948,\n",
    "        3949, 3950, 3951, 3952, 3953, 3954, 3973, 3974, 3975, 3976, 3977,\n",
    "        3978, 3979, 3980, 3981, 3982, 3983, 3984, 3985, 3986, 3987, 3988,\n",
    "        3989, 3990, 3991, 3992, 3993, 3994, 3995, 3996, 3997, 3998, 3999,\n",
    "        4000, 4001, 4002, 4003, 4004, 4005, 4006, 4007, 4008, 4009, 4010,\n",
    "        4011, 4012, 4013, 4014, 4015, 4016, 4017, 4018, 4019, 4020, 4021,\n",
    "        4022, 4035, 4036, 4037, 4038, 4039, 4040, 4041, 4042, 4043, 4044,\n",
    "        4045, 4046, 4047, 4048, 4049, 4050, 4051, 4052, 4053, 4054, 4055,\n",
    "        4056, 4057, 4058, 4059, 4060, 4061, 4062, 4063, 4064, 4065, 4066,\n",
    "        4067, 4068, 4069, 4070, 4071, 4072, 4073, 4074, 4075, 4076, 4077,\n",
    "        4078, 4079, 4080, 4081, 4082, 4083, 4084, 4085, 4086, 4087, 4088], dtype=int)\n",
    "    if OUTPUT_CHANNELS == 1:\n",
    "        filt = np.asarray([[[[1,   2, 1],\n",
    "                             [0,   0, 0],\n",
    "                             [-1, -2, -1]]]], dtype=float)\n",
    "    elif OUTPUT_CHANNELS == 2:\n",
    "        filt = np.asarray([[[[1,   2, 1],\n",
    "                             [0,   0, 0],\n",
    "                             [-1, -2, -1]]],\n",
    "                           [[[0.1111, 0.1111, 0.1111],\n",
    "                             [0.1111, 0.1111,  0.1111],\n",
    "                             [0.1111, 0.1111, 0.1111]]]], dtype=float)\n",
    "    else:\n",
    "        assert False\n",
    "elif INPUT_CHANNELS == 3:\n",
    "    spike_indices = np.asarray([197, 200, 203, 206, 209, 212, 215, 218, 221, 224,\n",
    "        227, 230, 233, 236, 239, 242, 245, 248, 251, 254,\n",
    "        257, 260, 263, 266, 269, 272, 275, 278, 281, 284,\n",
    "        287, 290, 293, 296, 299, 302, 305, 308, 311, 314,\n",
    "        317, 320, 323, 326, 329, 332, 389, 392, 395, 398,\n",
    "        401, 404, 407, 410, 413, 416, 419, 422, 425, 428,\n",
    "        431, 434, 437, 440, 443, 446, 449, 452, 455, 458,\n",
    "        461, 464, 467, 470, 473, 476, 479, 482, 485, 488,\n",
    "        491, 494, 497, 500, 503, 506, 509, 512, 515, 518,\n",
    "        521, 524, 581, 584, 587, 590, 593, 596, 599, 602,\n",
    "        605, 608, 611, 614, 617, 620, 623, 626, 629, 632,\n",
    "        635, 638, 641, 644, 647, 773, 776, 779, 782, 785,\n",
    "        788, 791, 794, 797, 800, 803, 806, 809, 812, 815,\n",
    "        818, 821, 824, 827, 830, 833, 836, 839, 965, 968,\n",
    "        971, 974, 977, 980, 983, 986, 989, 992, 995, 998,\n",
    "        1001, 1004, 1007, 1010, 1013, 1016, 1019, 1022, 1025, 1028,\n",
    "        1031, 1157, 1160, 1163, 1166, 1169, 1172, 1175, 1178, 1181,\n",
    "        1184, 1187, 1190, 1193, 1196, 1199, 1202, 1205, 1208, 1211,\n",
    "        1214, 1217, 1220, 1223, 1349, 1352, 1355, 1358, 1361, 1364,\n",
    "        1367, 1370, 1373, 1376, 1379, 1382, 1385, 1388, 1391, 1394,\n",
    "        1397, 1400, 1403, 1406, 1409, 1412, 1415, 1521, 1523, 1524,\n",
    "        1526, 1541, 1544, 1547, 1550, 1553, 1556, 1559, 1562, 1565,\n",
    "        1568, 1571, 1574, 1577, 1580, 1583, 1586, 1589, 1592, 1595,\n",
    "        1598, 1601, 1604, 1607, 1704, 1706, 1707, 1709, 1710, 1712,\n",
    "        1713, 1715, 1733, 1736, 1739, 1742, 1745, 1748, 1751, 1754,\n",
    "        1757, 1760, 1763, 1766, 1769, 1772, 1775, 1778, 1781, 1784,\n",
    "        1787, 1790, 1793, 1796, 1799, 1887, 1889, 1890, 1892, 1893,\n",
    "        1895, 1896, 1898, 1899, 1901, 1902, 1904, 1925, 1928, 1931,\n",
    "        1934, 1937, 1940, 1943, 1946, 1949, 1952, 1955, 1958, 1961,\n",
    "        1964, 1967, 1970, 1973, 1976, 1979, 1982, 1985, 1988, 1991,\n",
    "        2070, 2072, 2073, 2075, 2076, 2078, 2079, 2081, 2082, 2084,\n",
    "        2085, 2087, 2088, 2090, 2091, 2093, 2117, 2120, 2123, 2126,\n",
    "        2129, 2132, 2135, 2138, 2141, 2144, 2147, 2150, 2153, 2156,\n",
    "        2159, 2162, 2165, 2168, 2171, 2174, 2177, 2180, 2183, 2250,\n",
    "        2252, 2253, 2255, 2256, 2258, 2259, 2261, 2262, 2264, 2265,\n",
    "        2267, 2268, 2270, 2271, 2273, 2274, 2276, 2277, 2279, 2280,\n",
    "        2282, 2309, 2312, 2315, 2318, 2321, 2324, 2327, 2330, 2333,\n",
    "        2336, 2339, 2342, 2345, 2348, 2351, 2354, 2357, 2360, 2363,\n",
    "        2366, 2369, 2372, 2375, 2430, 2432, 2433, 2435, 2436, 2438,\n",
    "        2439, 2441, 2442, 2444, 2445, 2447, 2448, 2450, 2451, 2453,\n",
    "        2454, 2456, 2457, 2459, 2460, 2462, 2463, 2465, 2466, 2468,\n",
    "        2469, 2471, 2501, 2504, 2507, 2510, 2513, 2516, 2519, 2522,\n",
    "        2525, 2528, 2531, 2534, 2537, 2540, 2543, 2546, 2549, 2552,\n",
    "        2555, 2558, 2561, 2564, 2567, 2613, 2615, 2616, 2618, 2619,\n",
    "        2621, 2622, 2624, 2625, 2627, 2628, 2630, 2631, 2633, 2634,\n",
    "        2636, 2637, 2639, 2640, 2642, 2643, 2645, 2646, 2648, 2649,\n",
    "        2651, 2652, 2654, 2655, 2657, 2658, 2660, 2693, 2696, 2699,\n",
    "        2702, 2705, 2708, 2711, 2714, 2717, 2720, 2723, 2726, 2729,\n",
    "        2732, 2735, 2738, 2741, 2744, 2747, 2750, 2753, 2756, 2759,\n",
    "        2796, 2798, 2799, 2801, 2802, 2804, 2805, 2807, 2808, 2810,\n",
    "        2811, 2813, 2814, 2816, 2817, 2819, 2820, 2822, 2823, 2825,\n",
    "        2826, 2828, 2829, 2831, 2832, 2834, 2835, 2837, 2838, 2840,\n",
    "        2841, 2843, 2844, 2846, 2847, 2849, 2976, 2978, 2979, 2981,\n",
    "        2982, 2984, 2985, 2987, 2988, 2990, 2991, 2993, 2994, 2996,\n",
    "        2997, 2999, 3000, 3002, 3003, 3005, 3006, 3008, 3009, 3011,\n",
    "        3012, 3014, 3015, 3017, 3018, 3020, 3021, 3023, 3024, 3026,\n",
    "        3027, 3029, 3030, 3032, 3033, 3035, 3036, 3038, 3159, 3161,\n",
    "        3162, 3164, 3165, 3167, 3168, 3170, 3171, 3173, 3174, 3176,\n",
    "        3177, 3179, 3180, 3182, 3183, 3185, 3186, 3188, 3189, 3191,\n",
    "        3192, 3194, 3195, 3197, 3198, 3200, 3201, 3203, 3204, 3206,\n",
    "        3207, 3209, 3210, 3212, 3213, 3215, 3216, 3218, 3219, 3221,\n",
    "        3222, 3224, 3225, 3227, 3342, 3344, 3345, 3347, 3348, 3350,\n",
    "        3351, 3353, 3354, 3356, 3357, 3359, 3360, 3362, 3363, 3365,\n",
    "        3366, 3368, 3369, 3371, 3372, 3374, 3375, 3377, 3378, 3380,\n",
    "        3381, 3383, 3384, 3386, 3387, 3389, 3390, 3392, 3393, 3395,\n",
    "        3396, 3398, 3399, 3401, 3402, 3404, 3405, 3407, 3408, 3410,\n",
    "        3411, 3413, 3414, 3416, 3522, 3524, 3525, 3527, 3528, 3530,\n",
    "        3531, 3533, 3534, 3536, 3537, 3539, 3540, 3542, 3543, 3545,\n",
    "        3546, 3548, 3549, 3551, 3552, 3554, 3555, 3557, 3558, 3560,\n",
    "        3561, 3563, 3564, 3566, 3567, 3569, 3570, 3572, 3573, 3575,\n",
    "        3576, 3578, 3579, 3581, 3582, 3584, 3585, 3587, 3588, 3590,\n",
    "        3591, 3593, 3594, 3596, 3597, 3599, 3600, 3602, 3603, 3605,\n",
    "        3652, 3655, 3658, 3661, 3664, 3667, 3670, 3673, 3676, 3679,\n",
    "        3682, 3685, 3688, 3691, 3694, 3697, 3705, 3707, 3708, 3710,\n",
    "        3711, 3713, 3714, 3716, 3717, 3719, 3720, 3722, 3723, 3725,\n",
    "        3726, 3728, 3729, 3731, 3732, 3734, 3735, 3737, 3738, 3740,\n",
    "        3741, 3743, 3744, 3746, 3747, 3749, 3750, 3752, 3753, 3755,\n",
    "        3756, 3758, 3759, 3761, 3762, 3764, 3765, 3767, 3768, 3770,\n",
    "        3771, 3773, 3774, 3776, 3777, 3779, 3780, 3782, 3783, 3785,\n",
    "        3786, 3788, 3789, 3791, 3792, 3794, 3844, 3847, 3850, 3853,\n",
    "        3856, 3859, 3862, 3865, 3868, 3871, 3874, 3877, 3880, 3883,\n",
    "        3886, 3889, 3897, 3899, 3900, 3902, 3903, 3905, 3906, 3908,\n",
    "        3909, 3911, 3912, 3914, 3915, 3917, 3918, 3920, 3921, 3923,\n",
    "        3924, 3926, 3927, 3929, 3930, 3932, 3933, 3935, 3936, 3938,\n",
    "        3939, 3941, 3942, 3944, 3945, 3947, 3948, 3950, 3951, 3953,\n",
    "        3954, 3956, 3957, 3959, 3960, 3962, 3963, 3965, 3966, 3968,\n",
    "        3969, 3971, 3972, 3974, 3975, 3977, 3978, 3980, 3981, 3983,\n",
    "        3984, 3986, 4036, 4039, 4042, 4045, 4048, 4051, 4054, 4057,\n",
    "        4060, 4063, 4066, 4069, 4072, 4075, 4078, 4081, 4098, 4100,\n",
    "        4101, 4103, 4104, 4106, 4107, 4109, 4110, 4112, 4113, 4115,\n",
    "        4116, 4118, 4119, 4121, 4122, 4124, 4125, 4127, 4128, 4130,\n",
    "        4131, 4133, 4134, 4136, 4137, 4139, 4140, 4142, 4143, 4145,\n",
    "        4146, 4148, 4149, 4151, 4152, 4154, 4155, 4157, 4158, 4160,\n",
    "        4161, 4163, 4164, 4166, 4167, 4169, 4170, 4172, 4173, 4175,\n",
    "        4176, 4178, 4179, 4181, 4228, 4231, 4234, 4237, 4240, 4243,\n",
    "        4246, 4249, 4252, 4255, 4258, 4261, 4264, 4267, 4270, 4273,\n",
    "        4299, 4301, 4302, 4304, 4305, 4307, 4308, 4310, 4311, 4313,\n",
    "        4314, 4316, 4317, 4319, 4320, 4322, 4323, 4325, 4326, 4328,\n",
    "        4329, 4331, 4332, 4334, 4335, 4337, 4338, 4340, 4341, 4343,\n",
    "        4344, 4346, 4347, 4349, 4350, 4352, 4353, 4355, 4356, 4358,\n",
    "        4359, 4361, 4362, 4364, 4365, 4367, 4368, 4370, 4371, 4373,\n",
    "        4374, 4376, 4420, 4423, 4426, 4429, 4432, 4435, 4438, 4441,\n",
    "        4444, 4447, 4450, 4453, 4456, 4459, 4462, 4465, 4503, 4505,\n",
    "        4506, 4508, 4509, 4511, 4512, 4514, 4515, 4517, 4518, 4520,\n",
    "        4521, 4523, 4524, 4526, 4527, 4529, 4530, 4532, 4533, 4535,\n",
    "        4536, 4538, 4539, 4541, 4542, 4544, 4545, 4547, 4548, 4550,\n",
    "        4551, 4553, 4554, 4556, 4557, 4559, 4560, 4562, 4563, 4565,\n",
    "        4566, 4568, 4569, 4571, 4612, 4615, 4618, 4621, 4624, 4627,\n",
    "        4630, 4633, 4636, 4639, 4642, 4645, 4648, 4651, 4654, 4657,\n",
    "        4704, 4706, 4707, 4709, 4710, 4712, 4713, 4715, 4716, 4718,\n",
    "        4719, 4721, 4722, 4724, 4725, 4727, 4728, 4730, 4731, 4733,\n",
    "        4734, 4736, 4737, 4739, 4740, 4742, 4743, 4745, 4746, 4748,\n",
    "        4749, 4751, 4752, 4754, 4755, 4757, 4758, 4760, 4761, 4763,\n",
    "        4764, 4766, 4804, 4807, 4810, 4813, 4816, 4819, 4822, 4825,\n",
    "        4828, 4831, 4834, 4837, 4840, 4843, 4846, 4849, 4908, 4910,\n",
    "        4911, 4913, 4914, 4916, 4917, 4919, 4920, 4922, 4923, 4925,\n",
    "        4926, 4928, 4929, 4931, 4932, 4934, 4935, 4937, 4938, 4940,\n",
    "        4941, 4943, 4944, 4946, 4947, 4949, 4950, 4952, 4953, 4955,\n",
    "        4956, 4958, 4959, 4961, 4996, 4999, 5002, 5005, 5008, 5011,\n",
    "        5014, 5017, 5020, 5023, 5026, 5029, 5032, 5035, 5038, 5041,\n",
    "        5109, 5111, 5112, 5114, 5115, 5117, 5118, 5120, 5121, 5123,\n",
    "        5124, 5126, 5127, 5129, 5130, 5132, 5133, 5135, 5136, 5138,\n",
    "        5139, 5141, 5142, 5144, 5145, 5147, 5148, 5150, 5151, 5153,\n",
    "        5154, 5156, 5188, 5191, 5194, 5197, 5200, 5203, 5206, 5209,\n",
    "        5212, 5215, 5218, 5221, 5224, 5227, 5230, 5233, 5310, 5312,\n",
    "        5313, 5315, 5316, 5318, 5319, 5321, 5322, 5324, 5325, 5327,\n",
    "        5328, 5330, 5331, 5333, 5334, 5336, 5337, 5339, 5340, 5342,\n",
    "        5343, 5345, 5346, 5348, 5349, 5351, 5380, 5383, 5386, 5389,\n",
    "        5392, 5395, 5398, 5401, 5404, 5407, 5410, 5413, 5416, 5419,\n",
    "        5422, 5425, 5511, 5513, 5514, 5516, 5517, 5519, 5520, 5522,\n",
    "        5523, 5525, 5526, 5528, 5529, 5531, 5532, 5534, 5535, 5537,\n",
    "        5538, 5540, 5541, 5543, 5544, 5546, 5572, 5575, 5578, 5581,\n",
    "        5584, 5587, 5590, 5593, 5596, 5599, 5602, 5605, 5608, 5611,\n",
    "        5614, 5617, 5712, 5714, 5715, 5717, 5718, 5720, 5721, 5723,\n",
    "        5724, 5726, 5727, 5729, 5730, 5732, 5733, 5735, 5736, 5738,\n",
    "        5739, 5741, 5764, 5767, 5770, 5773, 5776, 5779, 5782, 5785,\n",
    "        5788, 5791, 5794, 5797, 5800, 5803, 5806, 5809, 5916, 5918,\n",
    "        5919, 5921, 5922, 5924, 5925, 5927, 5928, 5930, 5931, 5933,\n",
    "        5934, 5936, 5956, 5959, 5962, 5965, 5968, 5971, 5974, 5977,\n",
    "        5980, 5983, 5986, 5989, 5992, 5995, 5998, 6001, 6117, 6119,\n",
    "        6120, 6122, 6123, 6125, 6126, 6128, 6129, 6131, 6148, 6151,\n",
    "        6154, 6157, 6160, 6163, 6166, 6169, 6172, 6175, 6178, 6181,\n",
    "        6184, 6187, 6190, 6193, 6318, 6320, 6321, 6323, 6324, 6326,\n",
    "        6803, 6806, 6809, 6812, 6815, 6818, 6821, 6824, 6827, 6830,\n",
    "        6833, 6836, 6839, 6842, 6845, 6848, 6851, 6854, 6857, 6860,\n",
    "        6863, 6866, 6915, 6918, 6921, 6924, 6927, 6930, 6933, 6936,\n",
    "        6939, 6942, 6945, 6948, 6951, 6954, 6957, 6960, 6963, 6966,\n",
    "        6969, 6995, 6998, 7001, 7004, 7007, 7010, 7013, 7016, 7019,\n",
    "        7022, 7025, 7028, 7031, 7034, 7037, 7040, 7043, 7046, 7049,\n",
    "        7052, 7055, 7058, 7110, 7113, 7116, 7119, 7122, 7125, 7128,\n",
    "        7131, 7134, 7137, 7140, 7143, 7146, 7149, 7152, 7155, 7158,\n",
    "        7161, 7187, 7190, 7193, 7196, 7199, 7202, 7205, 7208, 7211,\n",
    "        7214, 7217, 7220, 7223, 7226, 7229, 7232, 7235, 7238, 7241,\n",
    "        7244, 7247, 7250, 7305, 7308, 7311, 7314, 7317, 7320, 7323,\n",
    "        7326, 7329, 7332, 7335, 7338, 7341, 7344, 7347, 7350, 7353,\n",
    "        7379, 7382, 7385, 7388, 7391, 7394, 7397, 7400, 7403, 7406,\n",
    "        7409, 7412, 7415, 7418, 7421, 7424, 7427, 7430, 7433, 7436,\n",
    "        7439, 7442, 7500, 7503, 7506, 7509, 7512, 7515, 7518, 7521,\n",
    "        7524, 7527, 7530, 7533, 7536, 7539, 7542, 7545, 7571, 7574,\n",
    "        7577, 7580, 7583, 7586, 7589, 7592, 7595, 7598, 7601, 7604,\n",
    "        7607, 7610, 7613, 7616, 7619, 7622, 7625, 7628, 7631, 7634,\n",
    "        7695, 7698, 7701, 7704, 7707, 7710, 7713, 7716, 7719, 7722,\n",
    "        7725, 7728, 7731, 7734, 7737, 7763, 7766, 7769, 7772, 7775,\n",
    "        7778, 7781, 7784, 7787, 7790, 7793, 7796, 7799, 7802, 7805,\n",
    "        7808, 7811, 7814, 7817, 7820, 7823, 7826, 7890, 7893, 7896,\n",
    "        7899, 7902, 7905, 7908, 7911, 7914, 7917, 7920, 7923, 7926,\n",
    "        7929, 7955, 7958, 7961, 7964, 7967, 7970, 7973, 7976, 7979,\n",
    "        7982, 7985, 7988, 7991, 7994, 7997, 8000, 8003, 8006, 8009,\n",
    "        8012, 8015, 8018, 8085, 8088, 8091, 8094, 8097, 8100, 8103,\n",
    "        8106, 8109, 8112, 8115, 8118, 8121, 8147, 8150, 8153, 8156,\n",
    "        8159, 8162, 8165, 8168, 8171, 8174, 8177, 8180, 8183, 8186,\n",
    "        8189, 8192, 8195, 8198, 8201, 8204, 8207, 8210, 8280, 8283,\n",
    "        8286, 8289, 8292, 8295, 8298, 8301, 8304, 8307, 8310, 8313,\n",
    "        8339, 8342, 8345, 8348, 8351, 8354, 8357, 8360, 8363, 8366,\n",
    "        8369, 8372, 8375, 8378, 8381, 8384, 8387, 8390, 8393, 8396,\n",
    "        8399, 8402, 8475, 8478, 8481, 8484, 8487, 8490, 8493, 8496,\n",
    "        8499, 8502, 8505, 8531, 8534, 8537, 8540, 8543, 8546, 8549,\n",
    "        8552, 8555, 8558, 8561, 8564, 8567, 8570, 8573, 8576, 8579,\n",
    "        8582, 8585, 8588, 8591, 8594, 8670, 8673, 8676, 8679, 8682,\n",
    "        8685, 8688, 8691, 8694, 8697, 8723, 8726, 8729, 8732, 8735,\n",
    "        8738, 8741, 8744, 8747, 8750, 8753, 8756, 8759, 8762, 8765,\n",
    "        8768, 8771, 8774, 8777, 8780, 8783, 8786, 8865, 8868, 8871,\n",
    "        8874, 8877, 8880, 8883, 8886, 8889, 8915, 8918, 8921, 8924,\n",
    "        8927, 8930, 8933, 8936, 8939, 8942, 8945, 8948, 8951, 8954,\n",
    "        8957, 8960, 8963, 8966, 8969, 8972, 8975, 8978, 9060, 9063,\n",
    "        9066, 9069, 9072, 9075, 9078, 9081, 9107, 9110, 9113, 9116,\n",
    "        9119, 9122, 9125, 9128, 9131, 9134, 9137, 9140, 9143, 9146,\n",
    "        9149, 9152, 9155, 9158, 9161, 9164, 9167, 9170, 9255, 9258,\n",
    "        9261, 9264, 9267, 9270, 9273, 9299, 9302, 9305, 9308, 9311,\n",
    "        9314, 9317, 9320, 9323, 9326, 9329, 9332, 9335, 9338, 9341,\n",
    "        9344, 9347, 9350, 9353, 9356, 9359, 9362, 9450, 9453, 9456,\n",
    "        9459, 9462, 9465, 9645, 9648, 9651, 9654, 9657, 9840, 9843,\n",
    "        9846, 9849, 10035, 10038, 10041, 10230, 10233, 10425, 10641, 10642,\n",
    "        10643, 10644, 10645, 10646, 10647, 10648, 10649, 10650, 10651, 10652,\n",
    "        10653, 10654, 10655, 10824, 10825, 10826, 10827, 10828, 10829, 10830,\n",
    "        10831, 10832, 10833, 10834, 10835, 10836, 10837, 10838, 10839, 10840,\n",
    "        10841, 10842, 10843, 10844, 10845, 10846, 10847, 10848, 10849, 10850,\n",
    "        10851, 10852, 10853, 10854, 10855, 10856, 11007, 11008, 11009, 11010,\n",
    "        11011, 11012, 11013, 11014, 11015, 11016, 11017, 11018, 11019, 11020,\n",
    "        11021, 11022, 11023, 11024, 11025, 11026, 11027, 11028, 11029, 11030,\n",
    "        11031, 11032, 11033, 11034, 11035, 11036, 11037, 11038, 11039, 11040,\n",
    "        11041, 11042, 11043, 11044, 11045, 11046, 11047, 11048, 11049, 11050,\n",
    "        11051, 11052, 11053, 11054, 11055, 11056, 11057, 11058, 11059, 11060,\n",
    "        11190, 11191, 11192, 11193, 11194, 11195, 11196, 11197, 11198, 11199,\n",
    "        11200, 11201, 11202, 11203, 11204, 11205, 11206, 11207, 11208, 11209,\n",
    "        11210, 11211, 11212, 11213, 11214, 11215, 11216, 11217, 11218, 11219,\n",
    "        11220, 11221, 11222, 11223, 11224, 11225, 11226, 11227, 11228, 11229,\n",
    "        11230, 11231, 11232, 11233, 11234, 11235, 11236, 11237, 11238, 11239,\n",
    "        11240, 11241, 11242, 11243, 11244, 11245, 11246, 11247, 11248, 11249,\n",
    "        11250, 11251, 11252, 11253, 11254, 11255, 11256, 11257, 11258, 11259,\n",
    "        11260, 11261, 11370, 11371, 11372, 11373, 11374, 11375, 11376, 11377,\n",
    "        11378, 11379, 11380, 11381, 11382, 11383, 11384, 11385, 11386, 11387,\n",
    "        11388, 11389, 11390, 11391, 11392, 11393, 11394, 11395, 11396, 11397,\n",
    "        11398, 11399, 11400, 11401, 11402, 11403, 11404, 11405, 11406, 11407,\n",
    "        11408, 11409, 11410, 11411, 11412, 11413, 11414, 11415, 11416, 11417,\n",
    "        11418, 11419, 11420, 11421, 11422, 11423, 11424, 11425, 11426, 11427,\n",
    "        11428, 11429, 11430, 11431, 11432, 11433, 11434, 11435, 11436, 11437,\n",
    "        11438, 11439, 11440, 11441, 11442, 11443, 11444, 11445, 11446, 11447,\n",
    "        11448, 11449, 11450, 11451, 11452, 11453, 11454, 11455, 11456, 11457,\n",
    "        11458, 11459, 11460, 11461, 11462, 11553, 11554, 11555, 11556, 11557,\n",
    "        11558, 11559, 11560, 11561, 11562, 11563, 11564, 11565, 11566, 11567,\n",
    "        11568, 11569, 11570, 11571, 11572, 11573, 11574, 11575, 11576, 11577,\n",
    "        11578, 11579, 11580, 11581, 11582, 11583, 11584, 11585, 11586, 11587,\n",
    "        11588, 11589, 11590, 11591, 11592, 11593, 11594, 11595, 11596, 11597,\n",
    "        11598, 11599, 11600, 11601, 11602, 11603, 11604, 11605, 11606, 11607,\n",
    "        11608, 11609, 11610, 11611, 11612, 11613, 11614, 11615, 11616, 11617,\n",
    "        11618, 11619, 11620, 11621, 11622, 11623, 11624, 11625, 11626, 11627,\n",
    "        11628, 11629, 11630, 11631, 11632, 11633, 11634, 11635, 11636, 11637,\n",
    "        11638, 11639, 11640, 11641, 11642, 11643, 11644, 11645, 11646, 11647,\n",
    "        11648, 11649, 11650, 11651, 11652, 11653, 11654, 11655, 11656, 11657,\n",
    "        11658, 11659, 11660, 11661, 11662, 11663, 11736, 11737, 11738, 11739,\n",
    "        11740, 11741, 11742, 11743, 11744, 11745, 11746, 11747, 11748, 11749,\n",
    "        11750, 11751, 11752, 11753, 11754, 11755, 11756, 11757, 11758, 11759,\n",
    "        11760, 11761, 11762, 11763, 11764, 11765, 11766, 11767, 11768, 11769,\n",
    "        11770, 11771, 11772, 11773, 11774, 11775, 11776, 11777, 11778, 11779,\n",
    "        11780, 11781, 11782, 11783, 11784, 11785, 11786, 11787, 11788, 11789,\n",
    "        11790, 11791, 11792, 11793, 11794, 11795, 11796, 11797, 11798, 11799,\n",
    "        11800, 11801, 11802, 11803, 11804, 11805, 11806, 11807, 11808, 11809,\n",
    "        11810, 11811, 11812, 11813, 11814, 11815, 11816, 11817, 11818, 11819,\n",
    "        11820, 11821, 11822, 11823, 11824, 11825, 11826, 11827, 11828, 11829,\n",
    "        11830, 11831, 11832, 11833, 11834, 11835, 11836, 11837, 11838, 11839,\n",
    "        11840, 11841, 11842, 11843, 11844, 11845, 11846, 11847, 11848, 11849,\n",
    "        11850, 11851, 11852, 11853, 11854, 11855, 11856, 11857, 11858, 11859,\n",
    "        11860, 11861, 11862, 11863, 11864, 11919, 11920, 11921, 11922, 11923,\n",
    "        11924, 11925, 11926, 11927, 11928, 11929, 11930, 11931, 11932, 11933,\n",
    "        11934, 11935, 11936, 11937, 11938, 11939, 11940, 11941, 11942, 11943,\n",
    "        11944, 11945, 11946, 11947, 11948, 11949, 11950, 11951, 11952, 11953,\n",
    "        11954, 11955, 11956, 11957, 11958, 11959, 11960, 11961, 11962, 11963,\n",
    "        11964, 11965, 11966, 11967, 11968, 11969, 11970, 11971, 11972, 11973,\n",
    "        11974, 11975, 11976, 11977, 11978, 11979, 11980, 11981, 11982, 11983,\n",
    "        11984, 11985, 11986, 11987, 11988, 11989, 11990, 11991, 11992, 11993,\n",
    "        11994, 11995, 11996, 11997, 11998, 11999, 12000, 12001, 12002, 12003,\n",
    "        12004, 12005, 12006, 12007, 12008, 12009, 12010, 12011, 12012, 12013,\n",
    "        12014, 12015, 12016, 12017, 12018, 12019, 12020, 12021, 12022, 12023,\n",
    "        12024, 12025, 12026, 12027, 12028, 12029, 12030, 12031, 12032, 12033,\n",
    "        12034, 12035, 12036, 12037, 12038, 12039, 12040, 12041, 12042, 12043,\n",
    "        12044, 12045, 12046, 12047, 12048, 12049, 12050, 12051, 12052, 12053,\n",
    "        12054, 12055, 12056, 12057, 12058, 12059, 12060, 12061, 12062, 12063,\n",
    "        12064, 12065, 12066, 12067, 12068, 12105, 12106, 12107, 12108, 12109,\n",
    "        12110, 12111, 12112, 12113, 12114, 12115, 12116, 12117, 12118, 12119,\n",
    "        12120, 12121, 12122, 12123, 12124, 12125, 12126, 12127, 12128, 12129,\n",
    "        12130, 12131, 12132, 12133, 12134, 12135, 12136, 12137, 12138, 12139,\n",
    "        12140, 12141, 12142, 12143, 12144, 12145, 12146, 12147, 12148, 12149,\n",
    "        12150, 12151, 12152, 12153, 12154, 12155, 12156, 12157, 12158, 12159,\n",
    "        12160, 12161, 12162, 12163, 12164, 12165, 12166, 12167, 12168, 12169,\n",
    "        12170, 12171, 12172, 12173, 12174, 12175, 12176, 12177, 12178, 12179,\n",
    "        12180, 12181, 12182, 12183, 12184, 12185, 12186, 12187, 12188, 12189,\n",
    "        12190, 12191, 12192, 12193, 12194, 12195, 12196, 12197, 12198, 12199,\n",
    "        12200, 12201, 12202, 12203, 12204, 12205, 12206, 12207, 12208, 12209,\n",
    "        12210, 12211, 12212, 12213, 12214, 12215, 12216, 12217, 12218, 12219,\n",
    "        12220, 12221, 12222, 12223, 12224, 12225, 12226, 12227, 12228, 12229,\n",
    "        12230, 12231, 12232, 12233, 12234, 12235, 12236, 12237, 12238, 12239,\n",
    "        12240, 12241, 12242, 12243, 12244, 12245, 12246, 12247, 12248, 12249,\n",
    "        12250, 12251, 12252, 12253, 12254, 12255, 12256, 12257, 12258, 12259,\n",
    "        12260, 12261, 12262, 12263, 12264, 12265, 12266,], dtype=int)\n",
    "\n",
    "    if OUTPUT_CHANNELS == 1:\n",
    "        filt = np.asarray([[[[1.0,  0.0,  -1.0],\n",
    "                             [2.0,  0.0,  -2.0],\n",
    "                             [1.0,  0.0,  -1.0]],\n",
    "                            [[1.0,  2.0,  1.0],\n",
    "                             [0.0,  0.0,  0.0],\n",
    "                             [-1.0, -2.0, -1.0]],\n",
    "                            [[-1.0, -1.0, -1.0],\n",
    "                             [-1.0, 8.0,  -1.0],\n",
    "                             [-1.0, -1.0, -1.0]]]], dtype=float)\n",
    "    elif OUTPUT_CHANNELS == 2:\n",
    "        filt = np.asarray([[[[1,   0, -1],\n",
    "                             [2,   0, -2],\n",
    "                             [1,   0, -1]],\n",
    "                            [[1,   2, 1],\n",
    "                             [0,   0, 0],\n",
    "                             [-1, -2, -1]],\n",
    "                            [[-1, -1, -1],\n",
    "                             [-1, 8,  -1],\n",
    "                             [-1, -1, -1]]],\n",
    "                           [[[0.1111, 0.1111, 0.1111],\n",
    "                             [0.1111, 0.1111,  0.1111],\n",
    "                             [0.1111, 0.1111, 0.1111]],\n",
    "                            [[0.1111, 0.1111, 0.1111],\n",
    "                             [0.1111, 0.1111,  0.1111],\n",
    "                             [0.1111, 0.1111, 0.1111]],\n",
    "                            [[0.1111, 0.1111, 0.1111],\n",
    "                             [0.1111, 0.1111,  0.1111],\n",
    "                             [0.1111, 0.1111, 0.1111]]]], dtype=float)\n",
    "    else:\n",
    "        assert False\n",
    "else:\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate \"full\" output size\n",
    "assert filt.shape[2] == filt.shape[3]\n",
    "CONV_KERNEL_SIZE = filt.shape[2]\n",
    "FULL_CONV_OUTPUT_SIZE = INPUT_SIZE + filt.shape[2] - 1\n",
    "\n",
    "# Same padding\n",
    "CONV_PAD = (CONV_KERNEL_SIZE - 1) // 2\n",
    "CONV_OUTPUT_SIZE = INPUT_SIZE\n",
    "\n",
    "# Valid padding\n",
    "#CONV_PAD = 0\n",
    "#CONV_OUTPUT_SIZE = INPUT_SIZE - CONV_KERNEL_SIZE + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Double but found Int",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-1c9e4b4536a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# Perform convolution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mconv_output_torch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_torch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCONV_PAD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mconv_output_torch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mCONV_OUTPUT_SIZE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mconv_output_torch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mCONV_OUTPUT_SIZE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected scalar type Double but found Int"
     ]
    }
   ],
   "source": [
    "# Turn spike indices into image\n",
    "# **NOTE** image is RGB\n",
    "image = np.zeros(INPUT_SIZE * INPUT_SIZE * INPUT_CHANNELS)\n",
    "image[spike_indices] = 1\n",
    "image = np.reshape(image, (INPUT_SIZE, INPUT_SIZE, INPUT_CHANNELS))\n",
    "\n",
    "# Reshape image into format expected by torch\n",
    "image_torch = np.rollaxis(image, 2)\n",
    "image_torch = np.expand_dims(image_torch, axis=0)\n",
    "\n",
    "# Perform convolution\n",
    "conv_output_torch = conv2d(torch.tensor(image_torch), torch.tensor(filt), padding=CONV_PAD).numpy()\n",
    "assert conv_output_torch.shape[2] == CONV_OUTPUT_SIZE\n",
    "assert conv_output_torch.shape[3] == CONV_OUTPUT_SIZE\n",
    "\n",
    "# Show results\n",
    "fig, axes = plt.subplots(OUTPUT_CHANNELS, 2, figsize=(16,4))\n",
    "for o in range(OUTPUT_CHANNELS):\n",
    "    axes_row = axes[o] if OUTPUT_CHANNELS > 1 else axes\n",
    "    axes_row[0].set_title(\"image\")\n",
    "    axes_row[0].imshow(image if INPUT_CHANNELS > 1 else image[:,:,0]);\n",
    "    \n",
    "    axes_row[1].set_title(\"filter output\")\n",
    "    axes_row[1].imshow(conv_output_torch[0,o,:,:], vmin=-10, vmax=10, interpolation=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toeplitz approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Zero CONV_PAD filter to output size\n",
    "zero_pad_filt = np.pad(np.flip(filt, axis=3), ((0, 0), (0, 0), (FULL_CONV_OUTPUT_SIZE - filt.shape[2], 0), (0, FULL_CONV_OUTPUT_SIZE - filt.shape[3])), \n",
    "                       \"constant\", constant_values=0.0)\n",
    "# Flip rows\n",
    "zero_pad_filt = np.flip(zero_pad_filt, axis=2)\n",
    "\n",
    "# Loop through output channels\n",
    "doubly_blocked_channels = []\n",
    "for o in range(zero_pad_filt.shape[0]):\n",
    "    doubly_blocked_channels.append([])\n",
    "    # Loop through input channels\n",
    "    for i in range(zero_pad_filt.shape[1]):\n",
    "        # Build list of toeplitz matrices from rows\n",
    "        f_matrices = []\n",
    "        for j in range(zero_pad_filt.shape[2]):\n",
    "            toeplitz_col = zero_pad_filt[o,i,j,:]\n",
    "            toeplitz_row = np.concatenate(((toeplitz_col[0],), np.zeros(INPUT_SIZE - 1)))\n",
    "            f_matrices.append(toeplitz(toeplitz_col, toeplitz_row))\n",
    "\n",
    "        # doubly blocked toeplitz indices: \n",
    "        # this matrix defines which toeplitz matrix from toeplitz_list goes to which part of the doubly blocked\n",
    "        block_toeplitz_col = list(range(1, zero_pad_filt.shape[2] + 1))\n",
    "        block_toeplitz_row = np.concatenate(((block_toeplitz_col[0],), np.zeros(INPUT_SIZE - 1, dtype=int)))\n",
    "        block_indices = toeplitz(block_toeplitz_col, block_toeplitz_row)\n",
    "\n",
    "        ## create doubly blocked matrix with zero values\n",
    "        toeplitz_shape = f_matrices[0].shape # shape of one toeplitz matrix\n",
    "\n",
    "        doubly_blocked = np.zeros((toeplitz_shape[0] * block_indices.shape[0], \n",
    "                                   toeplitz_shape[1] * block_indices.shape[1]))\n",
    "\n",
    "        # tile toeplitz matrices for each row in the doubly blocked matrix\n",
    "        for j in range(block_indices.shape[0]):\n",
    "            for k in range(block_indices.shape[1]):\n",
    "                start_j = j * toeplitz_shape[0]\n",
    "                start_k = k * toeplitz_shape[1]\n",
    "                end_j = start_j + toeplitz_shape[0]\n",
    "                end_k = start_k + toeplitz_shape[1]\n",
    "                doubly_blocked[start_j:end_j, start_k:end_k] = f_matrices[block_indices[j,k] - 1]\n",
    "\n",
    "        # If this is the first input and output channel\n",
    "        if i == 0 and o == 0:\n",
    "            # Show some example toeplitz matrices\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(16,2), sharey=True)\n",
    "            axes[0].imshow(f_matrices[0])\n",
    "            axes[0].set_title(\"Channel 0 (F0)\")\n",
    "            axes[1].imshow(f_matrices[1])\n",
    "            axes[1].set_title(\"Channel 0 (F1)\")\n",
    "            axes[2].imshow(f_matrices[2])\n",
    "            axes[2].set_title(\"Channel 0 (F2)\")\n",
    "\n",
    "            # Show complete, double-blocked matrix\n",
    "            fig, axis = plt.subplots(figsize=(16,16))\n",
    "            axis.imshow(doubly_blocked)\n",
    "            axis.set_title(\"Channel 0 (Doubly-blocked)\");\n",
    "\n",
    "        # Add double-blocked channel matrix to list\n",
    "        doubly_blocked_channels[-1].append(doubly_blocked)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard matrix multiply by Toeplitz\n",
    "I suspect the difference in sign is due to cross-correlation vs convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply each doubly-blocked channel matrix by appropriate image channel\n",
    "dense_result = [[doubly_blocked_channels[o][i].dot(image[:,:,i].flatten()) for i in range(INPUT_CHANNELS)] \n",
    "                for o in range(OUTPUT_CHANNELS)]\n",
    "\n",
    "# Sum across input channels\n",
    "sum_dense_result = np.sum(dense_result, axis=1)\n",
    "\n",
    "# Reshape and crop out output region from centre\n",
    "CONV_OUTPUT_CROP = (FULL_CONV_OUTPUT_SIZE - CONV_OUTPUT_SIZE) // 2\n",
    "sum_dense_result = np.reshape(sum_dense_result, (OUTPUT_CHANNELS, FULL_CONV_OUTPUT_SIZE, FULL_CONV_OUTPUT_SIZE))\n",
    "sum_dense_result = sum_dense_result[:,CONV_OUTPUT_CROP:-CONV_OUTPUT_CROP,CONV_OUTPUT_CROP:-CONV_OUTPUT_CROP]\n",
    "\n",
    "# Show results\n",
    "fig, axes = plt.subplots(OUTPUT_CHANNELS, 1 + INPUT_CHANNELS, figsize=(16,4))\n",
    "for o in range(OUTPUT_CHANNELS):\n",
    "    axes_row = axes[o] if OUTPUT_CHANNELS > 1 else axes\n",
    "    for i in range(INPUT_CHANNELS):\n",
    "        axes_row[i].set_title(\"input channel %u\" % i)\n",
    "        axes_row[i].imshow(np.reshape(dense_result[o][i], (FULL_CONV_OUTPUT_SIZE, FULL_CONV_OUTPUT_SIZE)), \n",
    "                           vmin=-10, vmax=10, interpolation=\"none\")\n",
    "\n",
    "    axes_row[-1].set_title(\"filter output\")\n",
    "    axes_row[-1].imshow(sum_dense_result[0], vmin=-10, vmax=10, interpolation=\"none\")\n",
    "\n",
    "assert np.allclose(sum_dense_result.flatten(), conv_output_torch.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse, spiking approach\n",
    "We're not actually multiplying - just summing weights from rows where there's spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_spike_output = np.zeros((OUTPUT_CHANNELS, CONV_OUTPUT_SIZE * CONV_OUTPUT_SIZE))\n",
    "\n",
    "# Loop through spikes\n",
    "for pre in spike_indices:\n",
    "    # Split pre into row, column and channel\n",
    "    pre_row = (pre // INPUT_CHANNELS) // INPUT_SIZE\n",
    "    pre_col = (pre // INPUT_CHANNELS) % INPUT_SIZE\n",
    "    pre_chan = pre % INPUT_CHANNELS\n",
    "    \n",
    "    # Loop through output channels\n",
    "    for o in range(OUTPUT_CHANNELS):\n",
    "        # Determine which column of blocks contains pre\n",
    "        # **NOTE** that the columns of the Toeplitz matrix are indexed rows\n",
    "        # Rotate row of zero-CONV_PADded filter corresponding to each block and concatenate together\n",
    "        col_test = np.concatenate([np.pad(zero_pad_filt[o, pre_chan, max(-1, i + CONV_OUTPUT_CROP - pre_row)], \n",
    "                                          (pre_col, 0))[:FULL_CONV_OUTPUT_SIZE]\n",
    "                                   for i in range(CONV_OUTPUT_SIZE)])\n",
    "        col_test = np.reshape(col_test, (CONV_OUTPUT_SIZE, FULL_CONV_OUTPUT_SIZE))\n",
    "        \n",
    "        # Add to output\n",
    "        sparse_spike_output[o] += col_test[:,CONV_OUTPUT_CROP:-CONV_OUTPUT_CROP].flatten()\n",
    "\n",
    "square_sparse_spike_output = np.reshape(sparse_spike_output, (OUTPUT_CHANNELS, CONV_OUTPUT_SIZE, CONV_OUTPUT_SIZE))\n",
    "\n",
    "fig, axes = plt.subplots(OUTPUT_CHANNELS)\n",
    "for o in range(OUTPUT_CHANNELS):\n",
    "    ax = axes[o] if OUTPUT_CHANNELS > 1 else axes\n",
    "    ax.imshow(square_sparse_spike_output[o], vmin=-10, vmax=10)\n",
    "    ax.set_title(\"Output %u\" % o);\n",
    "\n",
    "assert np.allclose(conv_output_torch.flatten(), square_sparse_spike_output.flatten())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparser, spiking approach\n",
    "There's no point in using the zero-CONV_PADded filters -  we can ignore zeroed rows and copy non-added columns into correct place rather than rotating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparser_spike_output = np.zeros((OUTPUT_CHANNELS, FULL_CONV_OUTPUT_SIZE * FULL_CONV_OUTPUT_SIZE))\n",
    "\n",
    "# Loop through spikes\n",
    "for pre in spike_indices:\n",
    "    # Split pre into row, column and channel\n",
    "    pre_row = (pre // INPUT_CHANNELS) // INPUT_SIZE\n",
    "    pre_col = (pre // INPUT_CHANNELS) % INPUT_SIZE\n",
    "    pre_chan = pre % INPUT_CHANNELS\n",
    "    \n",
    "    # Get length of filter row to apply here\n",
    "    row_len = min(filt.shape[3], FULL_CONV_OUTPUT_SIZE - pre_col)\n",
    "    \n",
    "    for o in range(OUTPUT_CHANNELS):\n",
    "        # Loop through filter rows\n",
    "        for i in range(pre_row, min(FULL_CONV_OUTPUT_SIZE, pre_row + filt.shape[2])):\n",
    "            # Extract row of filter\n",
    "            # **NOTE** from the POV of connectivity matrices, the columns of the Toeplitz matrix are the rows\n",
    "            filter_row_idx = filt.shape[2] - (i - pre_row) - 1\n",
    "            filter_row = filt[o, pre_chan, filter_row_idx]\n",
    "\n",
    "            # Place it into correct slice of output\n",
    "            start_out = (i * FULL_CONV_OUTPUT_SIZE) + pre_col\n",
    "            end_out = start_out + row_len\n",
    "            sparser_spike_output[o,start_out:end_out] += filter_row[-1:-row_len-1:-1]\n",
    "\n",
    "square_sparser_spike_output = np.reshape(sparser_spike_output, (OUTPUT_CHANNELS, FULL_CONV_OUTPUT_SIZE, FULL_CONV_OUTPUT_SIZE))\n",
    "square_sparser_spike_output = square_sparser_spike_output[:,CONV_OUTPUT_CROP:-CONV_OUTPUT_CROP,CONV_OUTPUT_CROP:-CONV_OUTPUT_CROP]\n",
    "\n",
    "fig, axes = plt.subplots(OUTPUT_CHANNELS)\n",
    "for o in range(OUTPUT_CHANNELS):\n",
    "    ax = axes[o] if OUTPUT_CHANNELS > 1 else axes\n",
    "    ax.imshow(square_sparser_spike_output[o], vmin=-10, vmax=10)\n",
    "    ax.set_title(\"Output %u\" % o);\n",
    "\n",
    "assert np.allclose(conv_output_torch.flatten(), square_sparser_spike_output.flatten())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparser, spiking approach rearranged for parallel implementation\n",
    "If we re-arrange the loops slightly we're closer to something that could be efficiently postsynaptically parallelised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparser_parallel_spike_output = np.zeros((OUTPUT_CHANNELS, CONV_OUTPUT_SIZE * CONV_OUTPUT_SIZE))\n",
    "\n",
    "# Parallelise across filter dimensions\n",
    "for kern_out_channel in range(filt.shape[0]):\n",
    "    for kern_row in range(filt.shape[2]):\n",
    "        for kern_col in range(filt.shape[3]):\n",
    "            # Extract vector of filter channels used by all spikes \n",
    "            filter_channels = filt[kern_out_channel,:,filt.shape[2] - kern_row - 1,filt.shape[3] - kern_col - 1]\n",
    "\n",
    "            # Loop through spikes\n",
    "            for pre in spike_indices:\n",
    "                # Split pre into row, column and channel\n",
    "                pre_row = (pre // INPUT_CHANNELS) // INPUT_SIZE\n",
    "                pre_col = (pre // INPUT_CHANNELS) % INPUT_SIZE\n",
    "                pre_chan = pre % INPUT_CHANNELS\n",
    "\n",
    "                # If we haven't gone off edge of output\n",
    "                post_row = pre_row + kern_row - CONV_OUTPUT_CROP\n",
    "                post_col = pre_col + kern_col - CONV_OUTPUT_CROP\n",
    "                if post_row >= 0 and post_row < CONV_OUTPUT_SIZE and post_col >= 0 and post_col < CONV_OUTPUT_SIZE:\n",
    "                    post_ind = (post_row * CONV_OUTPUT_SIZE) + post_col\n",
    "\n",
    "                    # Update output (coalesced reading of filter row and no collisions on atomic add)\n",
    "                    sparser_parallel_spike_output[kern_out_channel,post_ind] += filter_channels[pre_chan]\n",
    "\n",
    "square_sparser_parallel_spike_output = np.reshape(sparser_parallel_spike_output, (OUTPUT_CHANNELS, CONV_OUTPUT_SIZE, CONV_OUTPUT_SIZE))\n",
    "\n",
    "fig, axes = plt.subplots(OUTPUT_CHANNELS)\n",
    "for o in range(OUTPUT_CHANNELS):\n",
    "    ax = axes[o] if OUTPUT_CHANNELS > 1 else axes\n",
    "    ax.imshow(square_sparser_parallel_spike_output[o], vmin=-10, vmax=10)\n",
    "    ax.set_title(\"Output %u\" % o);\n",
    "\n",
    "assert np.allclose(conv_output_torch.flatten(), square_sparser_parallel_spike_output.flatten())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate \"full\" output size\n",
    "POOL_KERNEL_SIZE = 2\n",
    "POOL_STRIDE = 2\n",
    "POOL_CONV_INPUT_SIZE = int(np.ceil(float(INPUT_SIZE - POOL_KERNEL_SIZE + 1) / float(POOL_STRIDE)))\n",
    "FULL_POOL_CONV_OUTPUT_SIZE = POOL_CONV_INPUT_SIZE + filt.shape[2] - 1\n",
    "\n",
    "# Same convolution padding\n",
    "POOL_CONV_OUTPUT_SIZE = POOL_CONV_INPUT_SIZE\n",
    "\n",
    "# Valid convolution padding\n",
    "#POOL_CONV_OUTPUT_SIZE = POOL_CONV_INPUT_SIZE - CONV_KERNEL_SIZE + 1\n",
    "\n",
    "# Calculate required cropping\n",
    "POOL_OUTPUT_CROP = (FULL_POOL_CONV_OUTPUT_SIZE - POOL_CONV_OUTPUT_SIZE) // 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch test with pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuIAAAEICAYAAAAa1JFGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3de7CkdXng8e9zbnM7MBcFRMCgCWExVetgTbmk2LgIXtCo6JZmJYkhFsm4W5roalYJtVk1ZXYxFUW3kiWLQpgkRCQogRiiEoQ17maJw0VBSRakCIyMMyDMwMwwc27P/tHvkJ5D//r0uXS/3ed8P1WnTvf7ey/P6dOn+zm/9+nnjcxEkiRJUm8N1R2AJEmStBKZiEuSJEk1MBGXJEmSamAiLkmSJNXARFySJEmqgYm4JEmSVAMT8WUqIr4bEWfVHYckSctBRJwaEXdFxNMR8esR8YcR8VvV2FkRsaPuGDV4RuoOQN2RmT9VdwySJC0jHwJuy8zT51oxIh4CfiUz/6brUXUgIhI4JTMf6Mf9rWTOiEuSJM3tx4Dvdvsg0WB+tkL4i16mIuKhiHh1RHw0Iv48Iv60Op12T0T8ZET8ZkTsjohHIuK1Tdu9KyLuq9Z9MCLePWu/H4qInRHxaET8SkRkRPxENbYqIn4vIh6OiF3Vabs1vf7ZJUlaShHxdeBVwO9HxL7qffSqiPh4i3X/BHgR8JfVuh+qlp8REf8nIvZExLeby0cj4raI+J2I+N/AAeAlLfZ7WrXenqr89M2ztv+Vpvu/HBHfrG5/o1r87Sqef3e4lCYiLo6Ix6uc4RcWur95P6B6lon4yvAm4E+AjcBdwFdp/O5PAH4b+J9N6+4G3ggcDbwLuDQiXg4QEecCHwBeDfwE8G9mHecTwE8Cm6vxE4D/0pWfSJKkHsnMs4G/Bd6bmeOZ+f/arPtO4GHgTdW6vxsRJwB/BXwc2AT8BvDFiDimadN3AluBo4B/at5nRIwCfwl8DTgW+DXg6og4tYPYX1ndfFkVzxeq+y8Ank/jvfoC4PJF7k8LYCK+MvxtZn41M6eAPweOAS7JzEngGuDkiNgAkJl/lZnfz4b/ReOP/meq/fwc8EeZ+d3MPAB87PABIiKAXwX+Y2Y+kZlPA/8VeEevfkhJkvrULwI3ZeZNmTmTmTcD24E3NK1zVfX+OlW9Pzc7Axin8d49kZlfB74MnL/IuH4rMw9V7/d/ReN9Xj3khzVXhl1Nt58BHs/M6ab70PgD3xMRrwc+QmNmewhYC9xTrfNCGi8chz3SdPuYat07Gjk5AAEML9HPIEnSoPox4O0R8aamZaPArU33H6HshcAjmTnTtOyfaMxmL9STmbl/1v5euIj9aQFMxPWsiFgFfBH4JeCGzJyMiL+gkVAD7ARObNrkpKbbj9NI6n8qM3/Qi3glSepTOev+I8CfZOavzmObZo8CJ0XEUFMy/iLgcInMfhqTYYe9oIMYN0bEuqZk/EXAvYvYnxbA0hQ1GwNWAY8BU9Xs+Gubxq8F3lV9YGQtTfXf1QvDZ2nUlB8LEBEnRMTreha9JEn9YRdHfuDyT4E3RcTrImI4IlZXH5g8sbD9bLfTSI4/FBGj1Qc930SjvBTgbuDfRsTaqoHChXPEc9jHImIsIn6GxufD/nyR+9M8mYjrWVVd96/TSLifBH4euLFp/K+B/07jVNoDwN9VQ4eq7x+ulv/fiHgK+Btgzg9+SJK0zPw34D9XHU5+IzMfAc4DLqYx2fUI8J/oMA/LzAngzcDraZyB/h/AL2XmP1SrXApM0EiQtwFXz9rFR4FtVTyH68B/SOO9/tFq/X+/yP1pASKz3ZkQqSwiTqNxGmtV9UFQSZLU56oZ9T/NzE5n5NUlzohrXiLirdVprI002hX+pUm4JEnS/JmIa77eTeO02veBaeA/1BuOJEnSYLI0RZIkSarBombEI+LciPjHiHggIi5aqqAkSZKk5W7BM+IRMUyjf+VrgB3At4DzM/N7pW3GYlWuZt2CjidpbgfZz0QeirnXlKSGkbXrcnT9ppZj0eYTQLECT6hnm1fXbHf5Ol+Vj5CFaeDScmCgH8OpJ55get/+lj/BYi7o8wrggcx8ECAirqHRmqeYiK9mHf8qzlnEISW1c3veUncIkmoWEecCn6FxZePPZeYl7dYfXb+JF//yB1qOrf5ROdsePlQcWramV5XHJo8qZ4rTY10Ips+1S6qn1rZ+Xk2vbrO/kcH9z+/RT366OLaY0pQTOPJyrDtocanViNgaEdsjYvskK/CvVpKkHqnOVv8BjX7TLwXOj4iX1huVpJLFzIi3+tfvOf+uZOblwOUAR8emZ8e/+ujdizi0NH+ve+HmukOQpG6b99lqSfVZzIz4DuCkpvsn0rg6kyRJqse8z1ZPHdjfs+AkHWkxifi3gFMi4sURMQa8g6bLoUuSpJ7r+Gx1Zm7JzC0ja22iINVlwaUpmTkVEe8FvkrjAyFXZuZ3lywySZI0X56tlgbIYmrEycybgJuWKBZJkrQ4z56tBn5A42z1z7fbIIdgcrx1R4ocKncCiZmFBzmoSo8TwMRxk8WxkfHy2HI1M1N+7swcaJ1+xsFyoUZMD3D/wjYWlYhLkqT+4dlqabCYiEuStIx4tloaHCbiA8g2fJIkSYNvMV1TJEmSJC2QibgkSZJUAxNxSZIkqQbWiEuStILlMEwV2vKVlq9UOVZ+PGLVdA8j0VKIqXJLxKEFdJycGS0MtPkzckZckiRJqoGJuCRJklQDE3FJkiSpBibikiRJUg1MxCVJkqQa2DVFkiSpAzFR7rLBE2PFoZkuxDLIFjIL3K7DyfDB8nZje8vbjT1dbmcytbb1ds8cU94mRwpjbZ42zohLkiRJNTARlyRJkmpgIi5JkiTVwBpxqY989dG7i2Ove+HmHkYiSZK6zRlxSZIkqQYm4pIkSVINLE2ReqBdyUmJpSiSpEE1dKjcs290X+ux0f3l/Y22aTU4fKi83UyppSBw8HnlGA+c0Lrp5MyacjPKtu0tC5wRlyRJkmpgIi5JkiTVwERckiRJqoE14tIiLKT2ux3rwiVJWjmcEZckSZJqMGciHhFXRsTuiLi3admmiLg5Iu6vvm/sbpiSJEnS8tJJacpVwO8Df9y07CLglsy8JCIuqu5/eOnDk3pvqctN5mI5iiSpF2KqdXu94YPlbcb2llvyrdqzsJaCUN6uZKZNxnpoQznGfS8qH2vVS/YWx1YXlh94bF1xm5juQvvCzPwG8MSsxecB26rb24C3zPvIkiRpyUXEQxFxT0TcHRHb645HUtlCP6x5XGbuBMjMnRFxbGnFiNgKbAVYzdoFHk6SJM3DqzLz8bqDkNRe17umZOblwOUAR8em+Z+LkBah12UmnbAURZIkwcK7puyKiOMBqu+7ly4kSZK0CAl8LSLuqM5KP0dEbI2I7RGxfXpfm+uKS+qqhSbiNwIXVLcvAG5YmnAkSdIinZmZLwdeD7wnIl45e4XMvDwzt2TmluHx8ofPJHVXJ+0LPw/8HXBqROyIiAuBS4DXRMT9wGuq+5IkqWaZ+Wj1fTdwPfCKeiOSVDJnjXhmnl8YOmeJY5GK+rHWez6sC5fUCxGxDhjKzKer268FfrvtRkPJzLrp1kNrp8qbDa28j31NT7aZv9w3WhyKiQW0tTtU3mZ0X5uxNpVGo0+3/p0tdatBgOlV5bHJo1rHP9nm5MzE+nIcM8eW+y9u2Fh+QA5NltPgUpvCoWeW9lqYXuJekqTl4zjg+oiAxnv8n2XmV+oNSVKJibgkSctEZj4IvKzuOCR1xkRctRr0kpMSS1EkSdJclrbQRZIkSVJHTMQlSZKkGpiIS5IkSTWwRlxdsVxrv9uxLlzSQBpKRsYnWw4du+mp4mbjoxPdiqhvPba/3F9vz74NxbHVu1vPe47tLR9r5Jlyu76hqYW1FJwpZH0ThXaCABPry/tr21JwVbsYW4/lcJtt1rd+jsLStyiEpW9TWDxOT44iSZIk6Qgm4pIkSVINaitN8TT+YFiJJSbz4fNYkiQtlDPikiRJUg1MxCVJkqQa2DVlhbLkZGEsRZG07EwNMbNrdcuhHxaW95NSp40cW1hnkeGnh4tjqx8vdxfZ9KPy8YYPzT+W6VXlsYMbynFMlhuBMDneOo6ZVTOdhrUkit1RlmlnlHbqj0CSJElagUzEJUmSpBqYiEuSJEk1sEZ8mbH2e+lZFy5JkrrBGXFJkiSpBibikiRJUg0sTelTlpjUx1IUSSvJ0CSs+WHrebnVbVvydSui+Zle1bqV36E2Lf6m23RlHD5YHlu1Z2EtEQ8c1zqWifXl/bWLMUd6226wV3K6PD+896m1xbGZfaPFsX5oUdhOf0cnSZIkLVMm4pIkSVINTMQlSZKkGtRWI76ca6Db1Rgv5597kFkXLkmSes0ZcUmSJKkGcybiEXFSRNwaEfdFxHcj4n3V8k0RcXNE3F9939j9cCVJkqTloZPSlCngg5l5Z0QcBdwRETcDvwzckpmXRMRFwEXAh7sXqiRJWmo5BJPjrdvo5VC5BWD0SQe9LEwpTh5Vbg04uWm6ODa2sdy/cM34M8WxqTat9w7uW9N6mzZt9+JgeX8xXf69DIKhQ61/ttEnhttsU95fu9/19JqFtZzslTlnxDNzZ2beWd1+GrgPOAE4D9hWrbYNeEu3gpQkSf8sIq6MiN0RcW/TMs9USwNmXjXiEXEycDpwO3BcZu6ERrIOHFvYZmtEbI+I7ZP0Sfd/SZIG21XAubOWXUTjTPUpwC3VfUl9rONEPCLGgS8C78/MpzrdLjMvz8wtmblllFULiVGSJDXJzG8AT8xa7JlqacB01L4wIkZpJOFXZ+aXqsW7IuL4zNwZEccDu7sV5HLSaZs82xz2VvPjbStDSQPqiDPVEdHyTDU0zlYDWwFG1lvBItWlk64pAVwB3JeZn2oauhG4oLp9AXDD0ocnSZKWWvPZ6uF16+oOR1qxOilNORN4J3B2RNxdfb0BuAR4TUTcD7ymui9JkuqxqzpDjWeqpcEwZ2lKZn4TKPXJOWdpw9FhlrDUZ/ZjaqmKpAFx+Ez1JczjTHUOw1ShfWFp+SDI4TaxDy3s5xodLrc93Li63NrwmHX7Ww8cVz7Wvsmx4tie/a3bIQI8s6/N5/EK7RJjoj/aIcZUeWxsb3ls5GA5/meOKW/XD60NvbKmJEkDJiI+D/wdcGpE7IiIC/FMtTRwOvqwpiRJ6h+ZeX5hyDPV0gAxER9wCy2bsKSlc3ZUkSRJ3WBpiiRJklQDE3FJkiSpBibikiRJUg2sEV+hbI+4MLY2lKTBENPllnaxf7g4NrV/bXFs565y28BcPVMcGxmfbLn8qPFyy8P1aw4Wx07c0KaX34by0MRM65977zOri9vsfar8eOTecovFdi0Rc6R128CJ9cVNKHfShlVPlLda81h/tzZ0RlySJEmqgYm4JEmSVANLU9SW7RHbs7WhJElaKGfEJUmSpBqYiEuSJEk1MBGXJEmSamCNuLpiJbZHtF5ckpa3hbZEnCmM7d1Vbhu4Z6xN+7xCO0SANeOHimMb1rVul3jMuv3FbdqNTRxX/pkX0hJxpk07xIm2c8e9a2241G0NnRGXJEmSamAiLkmSJNXA0hTVarmWsHgFTkmSNBdnxCVJkqQamIhLkiRJNbA0RQNh0EtY7KgiSZqPmCh39OCJcneRg23Gdg6Pt1yeq2eK24ytL3dhef76fcWxBXViOb64Cfsmyz/X7ieOLo5NPrymOLbuB+XHuNRRpdRNBRbWUcUZcUmSJKkGJuKSJElSDUzEJUmSpBpYI65lZRBqyW1tKEmSwBlxSZIkqRZzJuIRsToi/j4ivh0R342Ij1XLXxwRt0fE/RHxhYgof5xVkiRJ0hE6KU05BJydmfsiYhT4ZkT8NfAB4NLMvCYi/hC4ELisi7FKS2ah5SDdKGmxtaGkWg0lM+umWw+tnSpvNjT/Vm3dMDPTus3czIFyihMHy/OQMd2mbeCAK/1ssX+4uM3U/rXFsZ27yq0BF9ISsV07xPHRifLYcY8XxyaOKf9sO160sThGoe3h0GR5k5gqPHfa/KnMOSOeDYcfmdHqK4Gzgeuq5duAt8y1L0mStHgRcWVE7I6Ie5uWfTQifhARd1dfb6gzRklz66hGPCKGI+JuYDdwM/B9YE9mHv5XeQdwQmHbrRGxPSK2T1JuCi9Jkjp2FXBui+WXZubm6uumHsckaZ46SsQzczozNwMnAq8ATmu1WmHbyzNzS2ZuGWXVwiOVJEkAZOY3gCfqjkPS4syrfWFm7omI24AzgA0RMVLNip8IPNqF+KS+0u32iNaLS1qk90bELwHbgQ9m5pOtVoqIrcBWgOFNG3oYnqRmnXRNOSYiNlS31wCvBu4DbgXeVq12AXBDt4KUJElzugz4cWAzsBP4ZGnF5rPVw0et61V8kmbpZEb8eGBbRAzTSNyvzcwvR8T3gGsi4uPAXcAVXYxTkiS1kZm7Dt+OiM8CX64xHEkdmDMRz8zvAKe3WP4gjXpxSbPMp6ykVMbiFTglzUdEHJ+ZO6u7bwXubbf+s4aSkfHWPdmO3fRUcbN27eR6ad9k68uY7H7i6OI205NtPrPWupPjnIqt64Dhg62XjxwobxPl7n8Dotw2MIdGWy5/fG357MzO9eUHJDaVn4vrjz5QHGv3/N6zqvXfxIHHyjEOPVMoNGnTEdNL3EuSNGAi4vPAWcDzI2IH8BHgrIjYTKN5wkPAu2sLUFJHTMQlSRowmXl+i8WWiEoDprZE3NPsUoN/C5IkrUwd9RGXJEmStLRMxCVJkqQamIhLkiRJNfDDmpIkrWQTQ/DwmpZDT35vbXGzva27u/VcFNoNjpe71pHlzno8c2wWx6bGy2OlOABG97XuX7f6R+X9DR8q72/QTRe6Rx58XrnP38xoee54prRDYO9j5bE9Y+XHP9cUfqFDbbYZLo+VOCMuSZIk1cBEXJIkSaqBpSnLWOmKjXouWwhKkqRec0ZckiRJqoGJuCRJklQDE3FJkiSpBrXViFu/bF2yJKkPRPt2fiWr9rRu1TbSpm1gv5g4qtwmb6HaPYaThbaHOVSOI2YWG1H/ysI08NTadq0Blz6OmGjz+E/0JkV2RlySJEmqgYm4JEmSVIPaSlNml2VYqiJJkqSVxBlxSZIkqQYm4pIkSVIN+ubKms2lKpapSJLUIyPJzLGHWg6tPnV/cbODk61TiAOPrStuM/RMv8z/lbtzLHiPI+V9To2Xli99HBos/fIXIUmSJK0oJuKSJElSDUzEJUmSpBr0TY14M1sbSpIkablzRlySJEmqQceJeEQMR8RdEfHl6v6LI+L2iLg/Ir4QEWPdC1OSJElaXuZTmvI+4D7g6Or+J4BLM/OaiPhD4ELgsiWOD7C1oSRJzSLiJOCPgRcAM8DlmfmZiNgEfAE4GXgI+LnMfLLtzqYD9o62HNpDuRXhho2F1obHlFseDkZrQ6l3OnrWR8SJwM8Cn6vuB3A2cF21yjbgLd0IUJIkPccU8MHMPA04A3hPRLwUuAi4JTNPAW6p7kvqU53++/lp4EM0/usGeB6wJzOnqvs7gBNabRgRWyNie0Rsn6T1BQMkSVLnMnNnZt5Z3X6axhnrE4DzaEyOgZNkUt+bMxGPiDcCuzPzjubFLVZteXmozLw8M7dk5pZRVi0wTEmS1EpEnAycDtwOHJeZO6GRrAPH1heZpLl0UiN+JvDmiHgDsJpGjfingQ0RMVLNip8IPNq9MP+Z9eKSJDVExDjwReD9mflUo3K0o+22AlsBhjdu6F6Aktqac0Y8M38zM0/MzJOBdwBfz8xfAG4F3latdgFwQ9eilCRJR4iIURpJ+NWZ+aVq8a6IOL4aPx7Y3Wrb5rPVw+PjvQlY0nMs5iPKHwY+EBEP0KgZv2JpQpIkSe1UTROuAO7LzE81Dd1IY3IMnCST+t68rqyZmbcBt1W3HwResfQhdc4rcEqSVqgzgXcC90TE4Te/i4FLgGsj4kLgYeDtnewspluXtGShrSGUWxsW2xqCrQ2lWfryEveSJKksM79J68YJAOf0MhZJC+e/mJIkSVINltWMuB1VJEmSNCicEZckSZJqYCIuSZIk1cBEXJIkSarBsqoRb2ZrQ0mSFqfU1hDKrQ1LbQ3B1obSbD57JUmSpBqYiEuSJEk1WLalKbPZ2lCSJEn9xBlxSZIkqQYm4pIkSVINTMQlSZKkGqyYGvFm1otLkrQ4pdaGpbaG0NvWhr1ua5jDWR5bPdNy+dDaqeI2Q0Pl/Q26mZnWz52ZA+W0NA6Wf5/t2mz2O2fEJUmSpBqYiEuSJEk1WJGlKc28AqckSZLq4Iy4JEmSVAMTcUmSJKkGK740ZTY7qkiStHDtOlj0sqNKqZsKdKmjynB5aGR8suXyYzc9VdxmfHRisRH1rX2TYy2X737i6OI205OryjucXmxE9XFGXJIkSaqBibgkSZJUAxNxSZIkqQbWiLcx6K0NZ8cvSZKk/uGMuCRJklSDjmbEI+Ih4Gkan0udyswtEbEJ+AJwMvAQ8HOZ+WR3wpQkSZKWl/mUprwqMx9vun8RcEtmXhIRF1X3P7yk0fUZWxtKkvpBRJwE/DHwAmAGuDwzPxMRHwV+FXisWvXizLypniifq6etDQttDaFLrQ3btNCb2tf6Z/vhzPpyHEO5sDgGwMxM6+fBzIFyWhoD3KKwncXUiJ8HnFXd3gbcxjJPxCVJ6hNTwAcz886IOAq4IyJursYuzczfqzE2SR3q9N++BL4WEXdExNZq2XGZuROg+n5sqw0jYmtEbI+I7ZMcWnzEkiStcJm5MzPvrG4/DdwHnFBvVJLmq9NE/MzMfDnweuA9EfHKTg+QmZdn5pbM3DJKm6siSZKkeYuIk4HTgdurRe+NiO9ExJURsbG2wCTNqaNEPDMfrb7vBq4HXgHsiojjAarvu7sVZD963Qs3P/slSVIdImIc+CLw/sx8CrgM+HFgM7AT+GRhu2fPVk/va3P5eEldNWciHhHrqvozImId8FrgXuBG4IJqtQuAG7oVpCRJOlJEjNJIwq/OzC8BZOauzJzOzBngszQmzp6j+Wz18Hj5g4uSuquTD2seB1wfEYfX/7PM/EpEfAu4NiIuBB4G3t69MCVJ0mHReFO+ArgvMz/VtPz4w5/fAt5KY+JMUp+aMxHPzAeBl7VY/iPgnG4ENWgG/QqckqSBcybwTuCeiDj8pnMxcH5EbKbRZOEh4N31hDd/S93asNjWELrS2nDoUHlseG/rsZED5Z8rh4tDTI6XWxvOrBrctocr8SqTXuJekqQBk5nfBFplrn3TM1zS3FbiPx+SJElS7UzEJUmSpBpYmtIFzTXj1otLkiSpFWfEJUmSpBqYiEuSJEk1sDSly2xtKEnS4iyktWGprSHA8OhMcaxdi8K1PyiPrdpTbhs4fKg0srBWgzNtsrepNeUYJ48qLG/TDnF6dflYOTK4rRL7hTPikiRJUg1MxCVJkqQaWJrSY7NLVbppKcpgehmvJEnSSuKMuCRJklQDE3FJkiSpBibikiRJUg2sER9AvWyB2OmxrCWXJHXL0KFy+8LRfa3nFIcfHi5uM3KgfKyRZ8ot+YamymPtWgpOHNU6/qm15W2Gpstjo0+X4xhrO1beZ8n0qvLYoQ3l+dyJ9fNvibgS2yE6Iy5JkiTVwERckiRJqoGlKX1kkK+6udDYLWmRJEkrlTPikiRJUg1MxCVJkqQamIhLkiRJNbBGvMsGue67F2yPKEnLS7tWg2N7242V97nQloIl7VoNTq0pxzixvrxdu3Z9M6tmOgmrYzFVjnH4YHm70X2ttxtt09aw3WO/dle7sfI+Sy0Rl7odIvR/S0RnxCVJkqQamIhLkiRJNbA0ZR4sM6mPJSySJGm5cUZckiRJqkFHiXhEbIiI6yLiHyLivoj46YjYFBE3R8T91feN3Q5WkiRJWi46LU35DPCVzHxbRIwBa4GLgVsy85KIuAi4CPhwl+LsGstNlhdLWCStBBGxGvgGsIrGe/l1mfmRiHgxcA2wCbgTeGdmTrTdWZa7cIzuL3fnWPWj1stH9y1thxMod9kAOLihdYyT68rbTI73rsNJN7TrBDI1Xt5uqvBzP/OC8ja97IKz0C4s7brgTI6X55wPPa/Ndutax7LUXVjmnBGPiKOBVwJXAGTmRGbuAc4DtlWrbQPesqSRSZKkkkPA2Zn5MmAzcG5EnAF8Arg0M08BngQurDFGSXPopDTlJcBjwB9FxF0R8bmIWAccl5k7Aarvx7baOCK2RsT2iNg+yaElC1ySpJUqG/ZVd0errwTOBq6rljtJJvW5ThLxEeDlwGWZeTqwn0YZSkcy8/LM3JKZW0Zpc25JkiR1LCKGI+JuYDdwM/B9YE9mTlWr7ABOqCs+SXPrpEZ8B7AjM2+v7l9HIxHfFRHHZ+bOiDiexgtB37D2W+3M5/lhPbmkfpSZ08DmiNgAXA+c1mq1VttGxFZgK8DwRnstSHWZc0Y8M38IPBIRp1aLzgG+B9wIXFAtuwC4oSsRSpKkoupzW7cBZwAbIuLwJNuJwKOFbZ49Wz28rs2nGiV1VaddU34NuLrqmPIg8C4aSfy1EXEh8DDw9u6EKEmSmkXEMcBkZu6JiDXAq2l8UPNW4G00Oqc4SSb1uY4S8cy8G9jSYuicpQ1nbpacqNcW8pyznEVSlx0PbIuIYaqJscz8ckR8D7gmIj4O3EXV8aytKLdkm1hfbtU2sb718hwrbxPry50U1x99oDy25mBxbGxoujim/rRvcqzl8sf3lnsvTuwtf84wDpYLPGK63GKxH3iJe0mSBkxmfgc4vcXyB4FX9D4iSQvhJe4lSZKkGpiIS5IkSTXoy9IU68A16No9h60flyRJ4Iy4JEmSVAsTcUmSJKkGkVluM7TkB4t4DPgn4PnA4z07cGv9EAMYx2zGcaT5xvFjmXlMt4KRtPw0vTfD4L72dYtxHMk4jtRpHMX35p4m4s8eNGJ7ZrbqS76iYjAO4xiUOCStDP3ymmMcxrFS4rA0RZIkSaqBibgkSZJUg7oS8ctrOm6zfogBjGM24zhSv8QhaWXol9cc4ziScRxp2cRRS424JEmStNJZmiJJkiTVwERckiRJqnTLVN0AAATDSURBVEFPE/GIODci/jEiHoiIi3p43CsjYndE3Nu0bFNE3BwR91ffN/YgjpMi4taIuC8ivhsR76sjlohYHRF/HxHfruL4WLX8xRFxexXHFyJirJtxNMUzHBF3RcSX64ojIh6KiHsi4u6I2F4tq+M5siEirouIf6ieJz9dRxySVp663qNbxPGc1+MeHbdfcoVWcXw0In5QPSZ3R8QbehBH7TlLmxjqeDy6kjv1LBGPiGHgD4DXAy8Fzo+Il/bo8FcB585adhFwS2aeAtxS3e+2KeCDmXkacAbwnuox6HUsh4CzM/NlwGbg3Ig4A/gEcGkVx5PAhV2O47D3Afc13a8rjldl5uamnqB1PEc+A3wlM/8F8DIaj0sdcUhaQWp+j25l9utxL1xFf+QKreKAxvvi5urrph7E0Q85SykG6P3j0ZXcqZcz4q8AHsjMBzNzArgGOK8XB87MbwBPzFp8HrCtur0NeEsP4tiZmXdWt5+mkWSd0OtYsmFfdXe0+krgbOC6XsUBEBEnAj8LfK66H3XEUdDT30tEHA28ErgCIDMnMnNPr+OQtCLV9h7dL/ooV2gVR8/1Q87SJoae61bu1MtE/ATgkab7O6jpwawcl5k7ofGLBo7t5cEj4mTgdOD2OmKpykHuBnYDNwPfB/Zk5lS1Sq9+P58GPgTMVPefV1McCXwtIu6IiK3Vsl7/Xl4CPAb8UVWq87mIWFdDHJJWnn56j271elyXfnr9fW9EfKcqXelpiWLdOUuLGKCGx6MbuVMvE/FosWxF9k6MiHHgi8D7M/OpOmLIzOnM3AycSGMm5LRWq3Uzhoh4I7A7M+9oXtzrOCpnZubLaZyWfU9EvLIHx5xtBHg5cFlmng7sxzIUSb3RT+/R/fB63G8uA36cRknETuCTvTpwP+QsLWKo5fHoRu7Uy0R8B3BS0/0TgUd7ePzZdkXE8QDV9929OGhEjNJ4Ml2dmV+qMxaAqvThNhq1VxsiYqQa6sXv50zgzRHxEI3ToGfTmCHvdRxk5qPV993A9TT+wHr9e9kB7MjMw//tX0cjMa/t+SFpxeib9+jC63Fd+uL1NzN3VUngDPBZevSY9EPO0iqGuh6Pw5Yyd+plIv4t4JTq06VjwDuAG3t4/NluBC6obl8A3NDtA1b1z1cA92Xmp+qKJSKOiYgN1e01wKtp1F3dCrytV3Fk5m9m5omZeTKN58PXM/MXeh1HRKyLiKMO3wZeC9xLj38vmflD4JGIOLVadA7wvV7HIWlF6ov36Davx3Xpi9ffw4lv5a304DHph5ylFENNj0dXcqeeXlmzai/zaWAYuDIzf6dHx/08cBbwfGAX8BHgL4BrgRcBDwNvz8yufjgiIv418LfAPfxzTfTFNOqdehZLRPxLGh8oGKbxz9i1mfnbEfESGjPTm4C7gF/MzEPdimNWTGcBv5GZb+x1HNXxrq/ujgB/lpm/ExHPo/fPkc00Prg6BjwIvIvqd9TLOCStPHW9R8+KoeXrcY+O3S+5Qqs4zqJRhpHAQ8C7D9dpdzGO2nOWNjGcT+8fj67kTl7iXpIkSaqBV9aUJEmSamAiLkmSJNXARFySJEmqgYm4JEmSVAMTcUmSJKkGJuKSJElSDUzEJUmSpBr8f9cnfAWdWgR3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1152x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Perform convolution\n",
    "pool_output_torch = conv2d(avg_pool2d(torch.tensor(image_torch), POOL_KERNEL_SIZE), torch.tensor(filt), padding=CONV_PAD).numpy()\n",
    "assert pool_output_torch.shape[2] == POOL_CONV_OUTPUT_SIZE\n",
    "assert pool_output_torch.shape[3] == POOL_CONV_OUTPUT_SIZE\n",
    "print(POOL_CONV_OUTPUT_SIZE)\n",
    "\n",
    "fig, axes = plt.subplots(OUTPUT_CHANNELS, 2, figsize=(16,4))\n",
    "for o in range(OUTPUT_CHANNELS):\n",
    "    axes_row = axes[o] if OUTPUT_CHANNELS > 1 else axes\n",
    "    axes_row[0].set_title(\"image\")\n",
    "    axes_row[0].imshow(image if INPUT_CHANNELS > 1 else image[:,:,0]);\n",
    "    \n",
    "    axes_row[1].set_title(\"filter output\")\n",
    "    axes_row[1].imshow(pool_output_torch[0,o,:,:], vmin=-10, vmax=10, interpolation=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparser, spiking approach rearranged for parallel implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAVwUlEQVR4nO3dfYxc1XnH8e8zs29er1l7ARuXl/AilIZUDYkshEoV5V0EqSLpSxQqIdKiOqqCVFrSCqVKQqr8kUQlaVRVtE5DIVEa8gYKqmgbhBKhqBHJhoJxAi2EEmIwNsbx4rW93p2Zp3/MdbJ27nN2PS93Zjm/jzTa2Xvm3nn27jx7Z88z5xxzd0Tkla826ABEpBpKdpFMKNlFMqFkF8mEkl0kE0p2kUwo2UUyoWRfw8zsfWb2mJkdMbMXzOw2M9t4Cvs/Y2Zv62E8Kx7PzN5qZk8UMX/bzF7Vq+eXNCX7GmVmNwGfBP4SmAYuB14F3G9mY4OMLWJmZwB3Ax8GZoBZ4CsDDSon7q7bGrsBpwHzwHtO2j4F7AP+uPj+DuDjy9rfBOwu7n8RaAFHi2P9FXA+4MB24HlgD3DTsv1P6XglcW8H/mvZ9+uLx//6oM9pDjdd2dem3wImaF8lf8Hd54F/B96+0gHc/VrgWeB33H3K3T+1rPnNwMXAO4CbV/NWf4XjHfda4NFl+xwGflJslz5Tsq9NZwD73b1R0ranaO/Gx9z9sLs/BvwLcE2XxztuCpg7adscsKFHx5cEJfvatB84w8xGStq2Fu3d+Nmy+z8Ffq3L4x03T/tfkOVOAw716PiSoGRfm74HHAN+d/lGM1sPvBN4oNh0GJhc9pCzTjpONOTx3GX3z6P9/3s3xzvuR8DrTor3omK79JmSfQ1y9zngY8Dfm9mVZjZqZucDXwN20+4sA3gEuMrMZszsLODGkw61F7iw5Ck+bGaTZvZa4I/4ZY95p8c77h7gN8zs98xsAvgIsNPdn1j5p5auDbqHULfOb8D1wC7aPdp7gX8CNi1rn6CdqC8DO4E/p+g9L9qvpt2pdhD4IL/aG/8Cy3rVT/V4QcxvA54oYv4OcP6gz2MuNyt+ASIU7w7+Dxj18s4/WcP0Nl4kE0p2kUzobbxIJnRlF8lE2Ycy+vdkk+t9dHqmtM0S3UGW4ZsPt0RbPbFjYr8ceXA5i7YDa/ocNg4coDl/uPQn6CrZzexK4LNAHfhnd/9E6vGj0zNc8L6/KG2beCnO6PqxLoJco5rjcdvShvjV2BzK8W79lUrcxmT566o5kTjeyNq9ujx/69+FbR2/jTezOvAPtD+xdQlwjZld0unxRKS/uvmf/TLgKXd/2t0Xgbtof6hCRIZQN8l+NicOmNhdbDuBmW03s1kzm20cOdzF04lIN7pJ9rJ/HH/lnx133+Hu29x928jk+i6eTkS60U2y7+bE0VHn8MvRUSIyZLrpjf8BcLGZXQA8B7wX+MPUDl6Dpanynk6vxT3M1uo8yLUqOk8Ai1uWwraRqbjtlarVil87rSPlL3FbiK9z1lzDtbeEjpPd3RtmdgPwn7RLb7e7u8Yliwyprurs7n4fcF+PYhGRPtLHZUUyoWQXyYSSXSQTSnaRTFQ66s3r0AhKStH2XPlYfD5svFlhJNIL1ojLebUOqqWt0aAhkUa6sotkQskukgklu0gmlOwimVCyi2Si0t54WT1bTAzGOBDPPZXhmKGkTq5mqZ7z+kK839hcvN/YobibvDFZvt/RM+N9wqmzEi8bXdlFMqFkF8mEkl0kE0p2kUwo2UUyoWQXyYRKb7Lm1Y7F9abR+fK20cSs5qOJMllqdaJWYiWZhdPjGI+cXV4wba2LC6nJ0mxAV3aRTCjZRTKhZBfJhJJdJBNKdpFMKNlFMqHSm/RFNHKs01Fj4wc7K4clJ2ULtBJZcWxjHOP8efFzjV84F7ZNBNuPvBgvhNrJElVdJbuZPQMcAppAw923dXM8EemfXlzZ3+zu+3twHBHpI/3PLpKJbpPdgW+Z2Q/NbHvZA8xsu5nNmtlscz7xGUUR6atu38Zf4e7Pm9lm4H4ze8LdH1z+AHffAewAGD/vXK0EITIgXV3Z3f354us+4B7gsl4EJSK91/GV3czWAzV3P1TcfwfwN8mdak5rffnSRbXJRrxbLb83BM2lxN/h+Wjtnw5HQ3Uwagw6GznW6zIZQHM8blvaUB7/UlzVYnE6jqO1Oa4dbtwUn5BjS3GqRSW22tHedql18zZ+C3CPmR0/zr+6+3/0JCoR6bmOk93dnwZe18NYRKSPVHoTyYSSXSQTSnaRTCjZRTJR7ai3mjMytVTatHnm5XC3qdHFfkU0tF48HNeGDs5vDNsm9pX//R6LB10xcjQuNdUanZXDopFji0EpDGBxOj5eshw2noqxvM3riX2my1+j0PvyGvS+xBY+TyXPIiIDp2QXyYSSXSQTSnaRTCjZRTJRbW98o0Zrb/mMWy8E24dJ1IPrY531WNcP1cO2if1xr/XMS6n52E49ltRAkoXEnGupwSRLU+VxtMbjJY36Iex1f4X2uKcMPgIRqYSSXSQTSnaRTCjZRTKhZBfJhJJdJBOVlt5qS7DuhfK/LxPJclK/Ijo1zfHyMlRqSaBmoqKYWgoptdxRypEt5bGkBpKkYvSRaktlVfFmfJ2be3kybGsl5v8bhvJaynBHJyI9o2QXyYSSXSQTSnaRTCjZRTKhZBfJRKWlN6/Fo6G8FpevbEiqPx78aVzaEJe1lmbKl7sCGNsU197WTR0N2xqJstHC/LryfVJLRi3Ex7PmqS8nNUxqx8p/ttED8YjDWqLUm/pdN9cN9zJlK17Zzex2M9tnZruWbZsxs/vN7Mni66b+hiki3VrN2/g7gCtP2nYz8IC7Xww8UHwvIkNsxWQv1ls/cNLmq4E7i/t3Au/qcVwi0mOddtBtcfc9AMXXzdEDzWy7mc2a2WzzcGKNXxHpq773xrv7Dnff5u7b6usT8xiJSF91mux7zWwrQPF1X+9CEpF+6LT0di9wHfCJ4us3V7OT16ERlN6i7WtBcimhWmc/12g9LtltmojLcmeuD/5V2hI/1/zSWNh28HB5KQ/g6Hxipsqg1GeLw1HKs0bcllwqayGO/+iZ8X7DUJZbTenty8D3gFeb2W4zu552kr/dzJ4E3l58LyJDbMUru7tfEzS9tcexiEgf6eOyIplQsotkQskukgklu0gmql3r7RUqNTLMDsejqxqH44kN9+yNS14+EQ8DHJkqX8NsQ2IU3fS6ePTdORsTdaiNcdNiq/znnjsaz26ZmujR5+LyYKqc5yPlJa/F6XAXID7e+MkfHF9m3YvDXZbTlV0kE0p2kUwo2UUyoWQXyYSSXSQTSnaRTKj0NqQ6Lee1gra5vXHJ6+BYovQTlPIA1k3FMzNuXF9e6gtH5a3Qtrgl/pk7Kee1EqW8xeQ1sLqyXK9Lcrqyi2RCyS6SCSW7SCaU7CKZULKLZEK98ZKeF+5A3Gu9kGjbU58q3Z4axDM2HffunzE9H7Z11MO/NdwlOSffvgOnhW1Lz8aDl9Y/F5/jqKe+14NndGUXyYSSXSQTSnaRTCjZRTKhZBfJhJJdJBPVlt5qTmt9+bJGtcl4PZ5ah0so9VqrVV4iaR2JT6MtxH9PU4Nd1rroZ6t6Tr6onJcq5U2NLsZtW/aHbYtnxj/b7vM2hW0EJbtaPAYJawSvndRKZHFTcVCz281sn5ntWrbtFjN7zsweKW5XrXQcERms1byNvwO4smT7Z9z90uJ2X2/DEpFeWzHZ3f1BIDFSV0TWgm466G4ws53F2/zwHxIz225ms2Y22zwUf6xRRPqr02S/DbgIuBTYA9waPdDdd7j7NnffVt+wvsOnE5FudZTs7r7X3Zvu3gI+B1zW27BEpNc6Kr2Z2VZ331N8+25gV+rxv1DzcHmizTMvh7ulSiFVikZDpUZCNZfG4wOWVyFXFJZdgHqwktPIkcScdnHlao2IS15eGy3dvn8yfpe5Zzo+ITYTvxanTzsStqVe3wfHy3PiyItxjLWjwXU6Uc1dMdnN7MvAm4AzzGw38FHgTWZ2Ke2q3jPA+1c6jogM1orJ7u7XlGz+fB9iEZE+0sdlRTKhZBfJhJJdJBNKdpFMVDvqbbEWjvD5+Y/jEU9zidE/VbKgVDYVV1zwuCrE0c3xEKXGVNwWxQEwOl9ee5l4KT5ePZ7ncc1rBpXPhdPjGlVrNL4GtqIDAnMvxm2pJbZ8XfALTYz29LomnBSRgJJdJBNKdpFMKNlFMqFkF8mEkl0kE9WW3ixdioqMHywvM4wkSl7DYnFD7yeVTJ3DpaBk57VX8qi3mAeXs8ZkqqzV+zhS6+nZYjVpqCu7SCaU7CKZULKLZELJLpIJJbtIJqrtjR9xWpvLR11MvDqeZnphqTzMjuboqlzvl67ykdQAmmj7cCyhJYMzLBkhIn2mZBfJhJJdJBNKdpFMKNlFMqFkF8nEalaEORf4AnAW0AJ2uPtnzWwG+ApwPu1VYd7j7j9PHqxpMFe+HM9B4jLaxk1BWe7MuFy3NspyItVZzau+Adzk7q8BLgc+YGaXADcDD7j7xcADxfciMqRWTHZ33+PuDxf3DwGPA2cDVwN3Fg+7E3hXv4IUke6d0vtZMzsfeD3wELDl+EquxdfNvQ5ORHpn1cluZlPAN4Ab3T1ef/ZX99tuZrNmNtucn+8kRhHpgVUlu5mN0k70L7n73cXmvWa2tWjfCuwr29fdd7j7NnffVp8KPrgtIn23YrKbmdFeovlxd//0sqZ7geuK+9cB3+x9eCLSK6sZ9XYFcC3wmJk9Umz7EPAJ4Ktmdj3wLPAHq3lCa5bPxeVBSQ7islxYkgOV5UROsmKyu/t3gWi2vLf2NhwR6RddxkQyoWQXyYSSXSQTSnaRTCjZRTJR7YSTCVFJDuKyXEcj5UBlOcmSXr0imVCyi2RCyS6SCSW7SCaU7CKZULKLZGJoSm8pvRwpB70vy1VdkvN6vG6bT7RKt9cmG+E+tdordx24Vqv8tdM6Er/0bSH+faZKxMNOV3aRTCjZRTKhZBfJhJJdJBNKdpFMrIne+Egng2eg9z31lQ+eqcdNI1NLpds3z8Szf0+NLnYb0dCaXxor3b7vwGnhPs2l8fiAzW4jGhxd2UUyoWQXyYSSXSQTSnaRTCjZRTKhZBfJxIqlNzM7F/gCcBbQAna4+2fN7BbgT4AXi4d+yN3v61egp6rSslzVc9olyj+N+fKf7YXWdByHBsKcwNZweS1lNXX2BnCTuz9sZhuAH5rZ/UXbZ9z9b/sXnoj0ymrWetsD7CnuHzKzx4Gz+x2YiPTWKb2PNLPzgdcDDxWbbjCznWZ2u5lt6nFsItJDq052M5sCvgHc6O4vA7cBFwGX0r7y3xrst93MZs1stjmf+CiqiPTVqpLdzEZpJ/qX3P1uAHff6+5Nd28BnwMuK9vX3Xe4+zZ331afijurRKS/Vkx2MzPg88Dj7v7pZdu3LnvYu4FdvQ9PRHplNb3xVwDXAo+Z2SPFtg8B15jZpYADzwDv70uEfdDrslzVS03VjsVt9bnytpEj8c/liVF0S1NxWa41vnZLdjl+wGQ1vfHfBcqyY2hq6iKyshz/wIlkSckukgklu0gmlOwimVCyi2RiTU842Q+dlOVSI+Xqo+XLMUG6vDb5XNw2fjAuedWPRS2dlclaiVdIY10c49KGYHuilNeciJ/LR9ZumW9Y6Moukgklu0gmlOwimVCyi2RCyS6SCSW7SCZUejtJ7VhcehudL//bWH82HjY2ciR+rpGjcTmp1kiMNkv81hY3lMffmIz3qSUmWBw9FMcxlmyLjxlpJpZYO7Yxvi4tTp96OS/HUp6u7CKZULKLZELJLpIJJbtIJpTsIplQsotkYk2X3lJlsrG5VFt8zE7LYZH0qLE4xsV4abZkqak1Ho+y64Q14hjrC/F+o/Pl+40mSnKpcz+5N9UWHzMq5/W6lAfDX87TlV0kE0p2kUwo2UUyoWQXyYSSXSQTK/bGm9kE8CAwXjz+6+7+UTO7ALgLmAEeBq5198XkwTzu3R09HPf6jr9Uvn10vrc955AejLGwsTzGpcR6lenlk3rbc94PqR7mxlS8XyP4uY+eFe9TZXWl0979VHVlaSq+dh47PbHf+vJYet27v5or+zHgLe7+OtrLM19pZpcDnwQ+4+4XAz8Hru9pZCLSUysmu7fNF9+OFjcH3gJ8vdh+J/CuvkQoIj2x2vXZ68UKrvuA+4GfAAfdvVE8ZDdwdn9CFJFeWFWyu3vT3S8FzgEuA15T9rCyfc1su5nNmtls83BiaWMR6atT6o1394PAd4DLgY1mdry74hzg+WCfHe6+zd231dcnerJEpK9WTHYzO9PMNhb31wFvAx4Hvg38fvGw64Bv9itIEeneagbCbAXuNLM67T8OX3X3fzOzHwN3mdnHgf8GPr/ikSwuJ6QGH0SDQnws3sem4yrg9GnxxHDT6+LRHWOpydpkKM0vjZVu3z8X1w0X5+L6qy3E18fU0mHDYMVkd/edwOtLtj9N+/93EVkD9Ak6kUwo2UUyoWQXyYSSXSQTSnaRTJh7dfNmmdmLwE+Lb88A9lf25DHFcSLFcaK1Fser3P3MsoZKk/2EJzabdfdtA3lyxaE4MoxDb+NFMqFkF8nEIJN9xwCfeznFcSLFcaJXTBwD+59dRKqlt/EimVCyi2RiIMluZlea2f+Y2VNmdvMgYijieMbMHjOzR8xstsLnvd3M9pnZrmXbZszsfjN7svi6aUBx3GJmzxXn5BEzu6qCOM41s2+b2eNm9iMz+7Nie6XnJBFHpefEzCbM7Ptm9mgRx8eK7ReY2UPF+fiKmZWP3424e6U3oE57DrsLgTHgUeCSquMoYnkGOGMAz/tG4A3ArmXbPgXcXNy/GfjkgOK4BfhgxedjK/CG4v4G4H+BS6o+J4k4Kj0ngAFTxf1R4CHas0N9FXhvsf0fgT89leMO4sp+GfCUuz/t7Xnm7wKuHkAcA+PuDwIHTtp8Ne1ZeqGi2XqDOCrn7nvc/eHi/iHaMyGdTcXnJBFHpbyt5zM6DyLZzwZ+tuz7Qc5M68C3zOyHZrZ9QDEct8Xd90D7RQdsHmAsN5jZzuJtft//nVjOzM6nPVnKQwzwnJwUB1R8Tvoxo/Mgkr1s7p5B1f+ucPc3AO8EPmBmbxxQHMPkNuAi2guC7AFureqJzWwK+AZwo7u/XNXzriKOys+JdzGjc2QQyb4bOHfZ9+HMtP3m7s8XX/cB9zDYabb2mtlWgOLrvkEE4e57ixdaC/gcFZ0TMxulnWBfcve7i82Vn5OyOAZ1TornPuUZnSODSPYfABcXPYtjwHuBe6sOwszWm9mG4/eBdwC70nv11b20Z+mFAc7Wezy5Cu+mgnNiZkZ7wtLH3f3Ty5oqPSdRHFWfk77N6FxVD+NJvY1X0e7p/Anw1wOK4ULalYBHgR9VGQfwZdpvB5dov9O5HjgdeAB4svg6M6A4vgg8BuyknWxbK4jjt2m/Jd0JPFLcrqr6nCTiqPScAL9Je8bmnbT/sHxk2Wv2+8BTwNeA8VM5rj4uK5IJfYJOJBNKdpFMKNlFMqFkF8mEkl0kE0p2kUwo2UUy8f87+1Ygrk1rigAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pool_sparser_parallel_spike_output = np.zeros((OUTPUT_CHANNELS, POOL_CONV_OUTPUT_SIZE * POOL_CONV_OUTPUT_SIZE))\n",
    "\n",
    "KERNEL_SCALE = 1.0 / (POOL_KERNEL_SIZE * POOL_KERNEL_SIZE)\n",
    "\n",
    "# Parallelise across filter dimensions\n",
    "for kern_out_channel in range(filt.shape[0]):\n",
    "    for kern_row in range(filt.shape[2]):\n",
    "        for kern_col in range(filt.shape[3]):\n",
    "            # Extract vector of filter channels used by all spikes \n",
    "            filter_channels = filt[kern_out_channel,:,filt.shape[2] - kern_row - 1,filt.shape[3] - kern_col - 1] * KERNEL_SCALE\n",
    "\n",
    "            # Loop through spikes\n",
    "            for pre in spike_indices:\n",
    "                # Split pre into row, column and channel going into pool\n",
    "                pool_pre_in_row = (pre // INPUT_CHANNELS) // INPUT_SIZE\n",
    "                pool_pre_in_col = (pre // INPUT_CHANNELS) % INPUT_SIZE\n",
    "                pool_pre_in_chan = pre % INPUT_CHANNELS\n",
    "                \n",
    "                # Calculate corresponding pool output\n",
    "                pool_pre_out_row = pool_pre_in_row // POOL_STRIDE\n",
    "                pool_stride_row = pool_pre_out_row * POOL_STRIDE\n",
    "                pool_pre_out_col = pool_pre_in_col // POOL_STRIDE\n",
    "                pool_stride_col = pool_pre_out_col * POOL_STRIDE\n",
    "                \n",
    "                if pool_pre_in_row < (pool_stride_row + POOL_KERNEL_SIZE) and pool_pre_in_col < (pool_stride_col + POOL_KERNEL_SIZE):\n",
    "                    # If we haven't gone off edge of output\n",
    "                    post_row = pool_pre_out_row + kern_row - POOL_OUTPUT_CROP\n",
    "                    post_col = pool_pre_out_col + kern_col - POOL_OUTPUT_CROP\n",
    "                    if post_row >= 0 and post_row < POOL_CONV_OUTPUT_SIZE and post_col >= 0 and post_col < POOL_CONV_OUTPUT_SIZE:\n",
    "                        post_ind = (post_row * POOL_CONV_OUTPUT_SIZE) + post_col\n",
    "\n",
    "                        # Update output (coalesced reading of filter row and no collisions on atomic add)\n",
    "                        pool_sparser_parallel_spike_output[kern_out_channel,post_ind] += filter_channels[pool_pre_in_chan]\n",
    "\n",
    "square_pool_sparser_parallel_spike_output = np.reshape(pool_sparser_parallel_spike_output, (OUTPUT_CHANNELS, POOL_CONV_OUTPUT_SIZE, POOL_CONV_OUTPUT_SIZE))\n",
    "\n",
    "fig, axes = plt.subplots(OUTPUT_CHANNELS)\n",
    "for o in range(OUTPUT_CHANNELS):\n",
    "    ax = axes[o] if OUTPUT_CHANNELS > 1 else axes\n",
    "    ax.imshow(square_pool_sparser_parallel_spike_output[o], vmin=-10, vmax=10, interpolation=\"none\")\n",
    "    ax.set_title(\"Output %u\" % o);\n",
    "\n",
    "assert np.allclose(pool_output_torch.flatten(), square_pool_sparser_parallel_spike_output.flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
